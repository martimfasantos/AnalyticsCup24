{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install them, you can uncomment the following lines:\n",
    "# (%pip will call pip from the currently active python environment)\n",
    "\n",
    "# Note: Some of these packages are still not compatible with Python 3.12 yet\n",
    "# %pip install sweetviz\n",
    "# %pip install ydata_profiling\n",
    "# %pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"font-weight: bold;\"> Analytics Cup 2024 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Note: The following do not work with Python 3.12\n",
    "import shap\n",
    "from ydata_profiling import ProfileReport\n",
    "import sweetviz as sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2024\n",
    "\n",
    "# pandas, statsmodels, matplotlib and y_data_profiling rely on numpy's random generator, and thus, we need to set the seed in numpy\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'> Phase 1: Business Understanding </font>\n",
    "\n",
    "Business Understanding is the first and economically most important step in the\n",
    "CRISP-DM process. It serves to assess use cases, feasibility, requirements, and\n",
    "risks of the endeavored data driven project. Since the conduction of data driven\n",
    "projects usually depends on the data at hand, the CRISP-DM process often \n",
    "alternates between Business Understanding and Data Understanding, until the\n",
    "project's schedule becomes sufficiently clear.\n",
    "\n",
    "#### Business Understanding\n",
    "\n",
    "In LLMeals Analytics Cup, the goal is to improve customer satisfaction and reduce subscription cancellations by developing a model that accurately predicts whether a customer likes (Like=1) or dislikes (Like=0) a suggested recipe. The model will utilize datasets such as \"recipes.csv,\" \"reviews.csv,\" \"diet.csv,\" and \"requests.csv\" to generate insights into user preferences. The successful model will serve as the foundation for enhancing the quality of suggested recipes in LLMeals' service, aligning them more closely with individual customer requirements. The project's ultimate aim is to leverage data-driven approaches for refining the recipe suggestions and, in turn, improving the overall LLMeals user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'> Phase 2: Data Understanding </font>\n",
    "\n",
    "The *Data Understanding* phase mainly serves to inform the Business Understanding step by\n",
    "assessing the data quality and content, and should provide the engineers with \n",
    "an intuition for the specific data and the specific problem at hand. Experienced\n",
    "data scientists and machine learning engineers can often estimate the difficulty\n",
    "and feasibility of the task by analyzing and understanding the data.  \n",
    "\n",
    "#### Data Understanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "file_dir = \"datasets/original\"\n",
    "file_names = [\"reviews.csv\", \"requests.csv\", \"diet.csv\", \"recipes.csv\"]\n",
    "reviews = pd.read_csv(f'{file_dir}/{file_names[0]}', low_memory=False)\n",
    "requests = pd.read_csv(f'{file_dir}/{file_names[1]}')\n",
    "diet = pd.read_csv(f'{file_dir}/{file_names[2]}')\n",
    "recipes = pd.read_csv(f'{file_dir}/{file_names[3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(reviews.sample(3))\n",
    "# print(\"\\n\")\n",
    "# print(reviews.info())\n",
    "# print(\"\\n\")\n",
    "# print(reviews.describe())\n",
    "# sns.boxplot(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(requests.sample(3))\n",
    "# print(\"\\n\")\n",
    "# print(requests.info())\n",
    "# print(\"\\n\")\n",
    "# print(requests.describe())\n",
    "# sns.boxplot(requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(diet.sample(3))\n",
    "# print(\"\\n\")\n",
    "# print(diet.info())\n",
    "# print(\"\\n\")\n",
    "# print(diet.describe())\n",
    "# sns.boxplot(diet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(recipes.sample(3))\n",
    "# print(\"\\n\")\n",
    "# print(recipes.info())\n",
    "# print(\"\\n\")\n",
    "# print(recipes.describe())\n",
    "# plt.figure(figsize=(24, 6))\n",
    "# sns.boxplot(recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the balancing of classes/labels\n",
    "# print(reviews.groupby(\"Like\").size())\n",
    "\n",
    "# # -> 2 classes, 1 is much more frequent than the other (False/True ratio is ~4:1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviews Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the feature distributions with a pairplot,\n",
    "# as it gives you a good overview over possible outliers\n",
    "# and a good overview over the data in general\n",
    "\n",
    "# pairplot for the full data\n",
    "# columns_to_drop = [\"AuthorId\", \"RecipeId\", \"TestSetId\"]\n",
    "# sns.pairplot(reviews.drop(columns_to_drop, axis=1), hue=\"Like\", diag_kind=\"hist\", diag_kws={\"multiple\" : \"stack\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requests Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the feature distributions with a pairplot,\n",
    "# as it gives you a good overview over possible outliers\n",
    "# and a good overview over the data in general\n",
    "\n",
    "# pairplot for the full data\n",
    "# columns_to_drop = [\"AuthorId\", \"RecipeId\"]\n",
    "# data = pd.merge(requests, reviews[[\"AuthorId\", \"RecipeId\", \"Like\"]], on=['AuthorId','RecipeId'], \\\n",
    "#     how='left').drop(columns_to_drop, axis=1)\n",
    "# sns.pairplot(data, hue=\"Like\", diag_kind=\"hist\", diag_kws={\"multiple\" : \"stack\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diet Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairplot for the full data\n",
    "# columns_to_drop = [\"AuthorId\"]\n",
    "# data = pd.merge(diet, reviews[[\"AuthorId\", \"Like\"]], on=['AuthorId'], how='left').drop(columns_to_drop, axis=1)\n",
    "# sns.pairplot(data, hue=\"Like\", diag_kind=\"hist\", diag_kws={\"multiple\" : \"stack\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recipes Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairplot for the full data\n",
    "# columns_to_drop = [\"RecipeId\"]\n",
    "# data = pd.merge(recipes, reviews[[\"RecipeId\", \"Like\"]], on=['RecipeId'], how='left').drop(columns_to_drop, axis=1)\n",
    "# sns.pairplot(data, hue=\"Like\", diag_kind=\"hist\", diag_kws={\"multiple\" : \"stack\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a merged dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **File**     | **Join Keys**            |\n",
    "|--------------|------------------------|\n",
    "| reviews.csv  | AuthorId, RecipeId     |\n",
    "| requests.csv | AuthorId, RecipeId     |\n",
    "| diet.csv     | AuthorId               |\n",
    "| recipes.csv  | RecipeId               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(140195, 5)\n",
      "(140195, 30)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Merge the dataframes using multiple columns\n",
    "merged_df = pd.merge(reviews, requests, on=['AuthorId','RecipeId'], how='left')\n",
    "merged_df = pd.merge(merged_df, diet, on='AuthorId', how='left')\n",
    "merged_df = pd.merge(merged_df, recipes, on='RecipeId', how='left')\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv('datasets/original/merged_data.csv', index=True)\n",
    "\n",
    "# Check if the number of rows and columns are correct\n",
    "print(len(merged_df) == len(reviews))\n",
    "print(reviews.shape)\n",
    "print(merged_df.shape)\n",
    "print(merged_df.shape[1] == reviews.shape[1] + requests.shape[1]-2 + diet.shape[1]-1 + recipes.shape[1]-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "file_dir = \"datasets/original\"\n",
    "file_name = \"merged_data.csv\"\n",
    "df = pd.read_csv(f'{file_dir}/{file_name}', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We can also leverage the dataprep package to get a nice summary report\n",
    "# report = sv.analyze(df)\n",
    "# report.show_notebook()\n",
    "\n",
    "# # We can also leverage the yadata_profiling package to get a nice summary report\n",
    "# profile = ProfileReport(df, title=\"LLMeals - Summary Report\")\n",
    "# profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: Data Understanding\n",
    "\n",
    "You should have a good understanding what the data is about and of some of its properties. Newly gained insights are used to reiterate the\n",
    "Business Understanding Phase, but in this example, it won't be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'> Phase 3: Data Preparation </font>\n",
    "\n",
    "Data Preparation mainly consists of two parts, Data Cleaning and Data Wrangling. In Data\n",
    "Cleaning, the goal is assure data quality. This includes removing wrong/corrupt \n",
    "data entries and making sure the entries are standardized, e.g. enforcing certain encodings. \n",
    "Data Wrangling then transforms the data in order to make it suitable for the modelling step.\n",
    "Sometimes, steps from Data Wrangling are incorporated into the automatized Pipeline, as\n",
    "we will show in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical vars:  ['AuthorId', 'Like', 'HighProtein', 'LowSugar', 'Diet', 'Name', 'RecipeCategory', 'RecipeIngredientQuantities', 'RecipeIngredientParts', 'RecipeYield']\n",
      "Boolean vars:  []\n",
      "Non-numeric vars:  ['AuthorId', 'Like', 'HighProtein', 'LowSugar', 'Diet', 'Name', 'RecipeCategory', 'RecipeIngredientQuantities', 'RecipeIngredientParts', 'RecipeYield']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "file_source_path = 'datasets/original/merged_data.csv' # source file\n",
    "file_dir = 'datasets/encoded' # destination directory\n",
    "file_tag = 'dataset'\n",
    "\n",
    "df = pd.read_csv(file_source_path, low_memory=False)\n",
    "index_column = df.columns[0]\n",
    "df.drop([index_column], axis=1, inplace=True)\n",
    "\n",
    "# ----------- Convertions ----------- #\n",
    "# Like: object -> bool\n",
    "# HighProtein: {Indiferent, Yes} - object -> bool\n",
    "# LowSugar: {0, Indiferent} - object -> bool\n",
    "# Diet: {Vegetarian, Omnivore, Vegan} - object -> categorical\n",
    "# Name: 140k values, 50% distinct - object -> categorical\n",
    "# RecipeCategory: 7 categories - object -> categorical\n",
    "# RecipeIngredientQuantities: filter\n",
    "# RecipeIngredientParts: filter\n",
    "# RecipeYield: 46k values, 7.9k distinct, 93.8k missing - object -> categorical\n",
    "# ------------------------------------------------------ #\n",
    "\n",
    "# AUX: encode the variables\n",
    "def encode_variable_types(df):\n",
    "    # Like: object -> bool\n",
    "    like_mapping = {False: 0, True: 1}\n",
    "    df[\"Like\"] = df[\"Like\"].replace(like_mapping).astype('category')\n",
    "    # HighProtein: {Indiferent, Yes} - object -> categorical\n",
    "    hp_mapping = {'Indifferent': 0, 'Yes': 1}\n",
    "    df[\"HighProtein\"] = df[\"HighProtein\"].replace(hp_mapping)\n",
    "    df[\"HighProtein\"] = df[\"HighProtein\"].astype('category')\n",
    "    # LowSugar: {0, Indiferent} - object -> categorical\n",
    "    ls_mapping = {'0': 0, 'Indifferent': 1}\n",
    "    df[\"LowSugar\"] = df[\"LowSugar\"].replace(ls_mapping)\n",
    "    df[\"LowSugar\"] = df[\"LowSugar\"].astype('category')\n",
    "    # Diet: {Vegetarian, Omnivore, Vegan} - object -> categorical\n",
    "    hp_diet = {'Vegetarian': 0, 'Omnivore': 1, 'Vegan': 2}\n",
    "    df[\"Diet\"] = df[\"Diet\"].replace(hp_diet)\n",
    "    df[\"Diet\"] = df[\"Diet\"].astype('category')\n",
    "    # Name: 140k values, 50% distinct - object -> categorical\n",
    "    df[\"Name\"] = df[\"Name\"].astype('category').cat.codes\n",
    "    df[\"Name\"] = df[\"Name\"].astype('category')\n",
    "    # RecipeCategory: 7 categories - object -> categorical\n",
    "    df[\"RecipeCategory\"] = df[\"RecipeCategory\"].astype('category').cat.codes\n",
    "    df[\"RecipeCategory\"] = df[\"RecipeCategory\"].astype('category')\n",
    "    # RecipeYield: 46k values, 7.9k distinct, 93.8k missing - object -> categorical\n",
    "    df[\"RecipeYield\"] = df[\"RecipeYield\"].astype('category').cat.codes.replace(-1, np.nan)\n",
    "    df[\"RecipeYield\"] = df[\"RecipeYield\"].astype('category')\n",
    "    # RecipeIngredientQuantities: filter?\n",
    "    df[\"RecipeIngredientQuantities\"] = df[\"RecipeIngredientQuantities\"].astype('category').cat.codes.replace(-1, np.nan)\n",
    "    df[\"RecipeIngredientQuantities\"] = df[\"RecipeIngredientQuantities\"].astype('category')\n",
    "    # RecipeIngredientParts: filter?\n",
    "    df[\"RecipeIngredientParts\"] = df[\"RecipeIngredientParts\"].astype('category').cat.codes.replace(-1, np.nan)\n",
    "    df[\"RecipeIngredientParts\"] = df[\"RecipeIngredientParts\"].astype('category')\n",
    "    # AuthorId: 140k values, 35% distinct - object -> categorical\n",
    "    df[\"AuthorId\"] = df[\"AuthorId\"].astype('category').cat.codes\n",
    "    df[\"AuthorId\"] = df[\"AuthorId\"].astype('category')\n",
    "    # Note: \"Rating\" was kept as float64 to allow for decimal values\n",
    "\n",
    "\n",
    "df_parts = df.copy(deep=True)\n",
    "\n",
    "# # convert all variables to the correct type\n",
    "encode_variable_types(df)\n",
    "\n",
    "# save the categorical variables\n",
    "categorical_vars = [column for column in df.columns if df[column].dtype.name == 'category']\n",
    "print(\"Categorical vars: \", categorical_vars)\n",
    "\n",
    "# save the boolean variables\n",
    "bool_vars = [column for column in df.columns if df[column].dtype.name == 'boolean']\n",
    "print(\"Boolean vars: \", bool_vars)\n",
    "\n",
    "# save the non-numeric variables\n",
    "non_numeric_vars = categorical_vars + bool_vars\n",
    "print(\"Non-numeric vars: \", non_numeric_vars)\n",
    "\n",
    "df.to_csv(f'{file_dir}/{file_tag}_encoded.csv', index=True)\n",
    "\n",
    "# ---------------------------------------------\n",
    "\n",
    "# import re\n",
    "\n",
    "# def parse_ingredient_parts(ingredient_parts):\n",
    "#     # Remove extra quotes around each ingredient\n",
    "    \n",
    "#     # Join the list of ingredients into a single string\n",
    "#     combined_string = ''.join(ingredient_parts)\n",
    "\n",
    "#     # Remove 'c(', ')' characters\n",
    "#     cleaned_string = combined_string.replace('c(', '').replace('\"', '').replace('\\\\', '').replace(')', '').split(',')\n",
    "\n",
    "#     # Split the string into a list of words\n",
    "#     parsed_ingredients = re.findall(r'\\b\\w+\\b', cleaned_string)\n",
    "#     print(cleaned_ingredients)\n",
    "#     return cleaned_ingredients\n",
    "\n",
    "# print(parse_ingredient_parts(df_parts['RecipeIngredientParts']))\n",
    "# df_parts['RecipeIngredientParts'] = df_parts['RecipeIngredientParts'].apply(parse_ingredient_parts)\n",
    "\n",
    "# # filter RecipeIngredientQuantities\n",
    "# import re\n",
    "# from fractions import Fraction\n",
    "\n",
    "# # Function to convert a string to a list of floats with fractions\n",
    "# def convert_to_float_array(value):\n",
    "#     clean_values = value.replace('c(', '').replace('\"', '').replace('\\\\', '').replace(')', '').split(',')\n",
    "#     result = []\n",
    "#     for item in clean_values:\n",
    "#         # Skip empty strings\n",
    "#         if item:\n",
    "#             # Check for mixed fractions with a space before them\n",
    "#             if ' ' in item:\n",
    "#                 whole_part, fraction_part = item.split(' ')\n",
    "#                 result.append(float(whole_part) + float(Fraction(fraction_part)))\n",
    "#             else:\n",
    "#                 result.append(float(Fraction(item)))\n",
    "#     return result\n",
    "\n",
    "# # df['RecipeIngredientQuantities'] = df['RecipeIngredientQuantities'].astype(str)\n",
    "# print(df['RecipeIngredientQuantities'].head())\n",
    "# # print(x for x in df.iloc[0]['RecipeIngredientQuantities'])\n",
    "\n",
    "# df['RecipeIngredientQuantities'] = df['RecipeIngredientQuantities'].apply(convert_to_float_array)\n",
    "\n",
    "# # Flatten the lists and remove None values\n",
    "# # df['RecipeIngredientQuantities'] = df['RecipeIngredientQuantities'].apply(lambda x: item for sublist in x \\\n",
    "# #                                                         if sublist is not None and sublist != [] for item in sublist)\n",
    "\n",
    "# df['RecipeIngredientQuantities'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------------------------------------------- #\n",
    "# # NOTE IMPORTANT: call this function after loading the dataframe #\n",
    "# # --------------------------------------------------------- #\n",
    "def convert_variable_types(df):\n",
    "    df[\"Like\"] = df[\"Like\"].astype('category')\n",
    "    df[\"HighProtein\"] = df[\"HighProtein\"].astype('category')\n",
    "    df[\"LowSugar\"] = df[\"LowSugar\"].astype('category')\n",
    "    df[\"Diet\"] = df[\"Diet\"].astype('category')\n",
    "    df[\"Name\"] = df[\"Name\"].astype('category')\n",
    "    df[\"RecipeCategory\"] = df[\"RecipeCategory\"].astype('category')\n",
    "    df[\"RecipeIngredientQuantities\"] = df[\"RecipeIngredientQuantities\"].astype('category')\n",
    "    df[\"RecipeIngredientParts\"] = df[\"RecipeIngredientParts\"].astype('category')\n",
    "    df[\"RecipeYield\"] = df[\"RecipeYield\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUX: Get variable types\n",
    "def get_variable_types(df) -> dict:\n",
    "    variable_types: dict = {\n",
    "        'Numeric': [],\n",
    "        'Binary': [],\n",
    "        'Categorical': []\n",
    "    }\n",
    "\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == 'boolean':\n",
    "            variable_types['Binary'].append(c)\n",
    "        elif df[c].dtype == 'int8' or df[c].dtype == 'int16' or df[c].dtype == 'int32' or df[c].dtype == 'int64' or \\\n",
    "            df[c].dtype == 'float16' or df[c].dtype == 'float32' or df[c].dtype == 'float64':\n",
    "            variable_types['Numeric'].append(c)\n",
    "        elif df[c].dtype == 'category' or df[c].dtype == 'object':\n",
    "            variable_types['Categorical'].append(c)\n",
    "        else:\n",
    "            print(f'Unknown variable type for {c}')\n",
    "\n",
    "    return variable_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "Rating : 63087 (45.0%)\n",
      "Like : 42814 (30.54%)\n",
      "TestSetId : 97381 (69.46%)\n",
      "RecipeServings : 50021 (35.68%)\n",
      "RecipeYield : 93888 (66.97%)\n"
     ]
    }
   ],
   "source": [
    "# fill/remove/change missing/corrupt values\n",
    "from pandas import concat, DataFrame\n",
    "from sklearn.impute import SimpleImputer\n",
    "from numpy import nan\n",
    "\n",
    "# file_source_path = 'datasets/original/merged_data.csv' # source file\n",
    "file_source_path = 'datasets/encoded/dataset_encoded.csv' # source file\n",
    "file_dir = 'datasets/missing_values' # destination directory\n",
    "file_tag = 'dataset'\n",
    "\n",
    "# Import data\n",
    "df = pd.read_csv(file_source_path, low_memory=False)\n",
    "convert_variable_types(df)\n",
    "index_column = df.columns[0]\n",
    "df.drop([index_column], axis=1, inplace=True)\n",
    "\n",
    "# --------------- #\n",
    "# Missing Values  #\n",
    "# --------------- #\n",
    "\n",
    "print(\"Missing values:\")\n",
    "mv = {}\n",
    "for var in df:\n",
    "    nr = df[var].isna().sum()\n",
    "    if nr > 0:\n",
    "        mv[var] = nr\n",
    "        print(f\"{var} : {nr} ({round(nr/df[var].shape[0]*100, 2)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines the number of records to discard entire COLUMNS\n",
    "threshold = df.shape[0] * 0.90\n",
    "\n",
    "# drop columns with more missing values than the defined threshold\n",
    "missings = [c for c in mv.keys() if mv[c]>threshold]\n",
    "df = df.drop(columns=missings, inplace=False)\n",
    "\n",
    "# remove meaningless columns (after manual inspection)\n",
    "df = df.drop(columns=['Rating'], inplace=False)\n",
    "# df = df.drop(columns=['RecipeYield'], inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 1**: Fill with CONSTANT Value after DROP Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------- #\n",
    "# APPROACH 1: Fill with CONSTANT Value after DROP Missing Values #\n",
    "# -------------------------------------------------------------- #\n",
    "\n",
    "# AUX: Fill with CONSTANT value\n",
    "def fill_with_constant(data: DataFrame) -> DataFrame:\n",
    "    tmp_nr, tmp_cat, tmp_bool = None, None, None\n",
    "    variables = get_variable_types(data.drop(['TestSetId', 'Like'], axis=1))\n",
    "    numeric_vars = variables['Numeric']\n",
    "    categorical_vars = variables['Categorical']\n",
    "    binary_vars = variables['Binary']\n",
    "    \n",
    "    # print(f'Numeric variables: {numeric_vars}')\n",
    "    # print(f'Categorical variables: {categorical_vars}')\n",
    "    # print(f'Binary variables: {binary_vars}')\n",
    "        \n",
    "    if len(numeric_vars) > 0:\n",
    "        imp = SimpleImputer(strategy='constant', fill_value=0, missing_values=nan, copy=True)\n",
    "        tmp_nr = DataFrame(imp.fit_transform(data[numeric_vars]), columns=numeric_vars)\n",
    "    if len(categorical_vars) > 0:\n",
    "        imp = SimpleImputer(strategy='constant', fill_value=-1, missing_values=nan, copy=True)\n",
    "        tmp_cat = DataFrame(imp.fit_transform(data[categorical_vars]), columns=categorical_vars)\n",
    "    if len(binary_vars) > 0:\n",
    "        imp = SimpleImputer(strategy='constant', fill_value=False, missing_values=nan, copy=True)\n",
    "        tmp_bool = DataFrame(imp.fit_transform(data[binary_vars].astype(int)), columns=binary_vars).astype(bool)\n",
    "\n",
    "    df = concat([tmp_nr, tmp_cat, tmp_bool], axis=1)\n",
    "    df['TestSetId'] = data['TestSetId'] ; df['Like'] = data['Like']\n",
    "    df.index = data.index\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------------------------------------------- #\n",
    "\n",
    "# df.info()\n",
    "\n",
    "# Fill the rest with constant\n",
    "df_const = fill_with_constant(df)\n",
    "df_const.to_csv(f'{file_dir}/{file_tag}_drop_columns_then_constant.csv', index=True)\n",
    "# df_const.head()\n",
    "\n",
    "# Best results: Training Score = 0.557 ; Test Score = 0.555"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 2**: Fill with MOST FREQ Value after DROP Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------- #\n",
    "# APPROACH 2: Fill with MOST FREQ Value after DROP Missing Values #\n",
    "# --------------------------------------------------------------- #\n",
    "\n",
    "# AUX: Fill with MOST FREQUENT value\n",
    "def fill_with_most_frequent(data: DataFrame) -> DataFrame:\n",
    "    tmp_nr, tmp_cat, tmp_bool = None, None, None\n",
    "    variables = get_variable_types(data.drop(['TestSetId','Like'], axis=1))\n",
    "    numeric_vars = variables['Numeric']\n",
    "    categorical_vars = variables['Categorical']\n",
    "    binary_vars = variables['Binary']\n",
    "\n",
    "    tmp_nr, tmp_cat, tmp_bool = None, None, None\n",
    "    if len(numeric_vars) > 0:\n",
    "        imp = SimpleImputer(strategy='mean', missing_values=nan, copy=True)\n",
    "        tmp_nr = DataFrame(imp.fit_transform(data[numeric_vars]), columns=numeric_vars)\n",
    "    if len(categorical_vars) > 0:\n",
    "        imp = SimpleImputer(strategy='most_frequent', missing_values=nan, copy=True)\n",
    "        tmp_cat = DataFrame(imp.fit_transform(data[categorical_vars]), columns=categorical_vars)\n",
    "    if len(binary_vars) > 0:\n",
    "        imp = SimpleImputer(strategy='most_frequent', missing_values=nan, copy=True)\n",
    "        tmp_bool = DataFrame(imp.fit_transform(data[binary_vars].astype(int)), columns=binary_vars).astype(bool)\n",
    "\n",
    "    df = concat([tmp_nr, tmp_cat, tmp_bool], axis=1)\n",
    "    df['TestSetId'] = data['TestSetId'] ; df['Like'] = data['Like']\n",
    "    df.index = data.index\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------------------------------------------- #\n",
    "\n",
    "# Fill the rest with most frequent value\n",
    "df_most_freq = fill_with_most_frequent(df)\n",
    "df_most_freq.to_csv(f'{file_dir}/{file_tag}_drop_columns_then_most_frequent.csv', index=True)\n",
    "# df_most_freq.head()\n",
    "\n",
    "# Best results: Training Score = 0.557 ; Test Score = 0.569"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Wrangling\n",
    "\n",
    "In contrast to Data Cleaning, Data Wrangling _transforms_ the dataset, in order\n",
    "to prepare it for the training of the models. This includes scaling, dimensionality\n",
    "reduction, data augmentation, outlier removal, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric vars:  ['RecipeId', 'Time', 'Age', 'CookTime', 'PrepTime', 'Calories', 'FatContent', 'SaturatedFatContent', 'CholesterolContent', 'SodiumContent', 'CarbohydrateContent', 'FiberContent', 'SugarContent', 'ProteinContent', 'RecipeServings']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "p-value may not be accurate for N > 5000.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "# Best option: dataset_rating_drop_recipe_mean\n",
    "file_source_path = 'datasets/missing_values/dataset_drop_columns_then_most_frequent.csv' # source file\n",
    "file_dir = 'datasets/outliers' # destination directory\n",
    "file_tag = 'dataset'\n",
    "\n",
    "# read the data\n",
    "df = pd.read_csv(file_source_path, low_memory=False)\n",
    "convert_variable_types(df)\n",
    "index_column = df.columns[0]\n",
    "df.drop([index_column], axis=1, inplace=True)\n",
    "\n",
    "# print(df.info())\n",
    "\n",
    "non_numeric_vars =  ['TestSetId', 'AuthorId', 'Diet', 'Name', 'RecipeCategory', 'RecipeIngredientQuantities', \\\n",
    "                     'RecipeIngredientParts', 'RecipeYield', 'Like', 'HighCalories', 'HighProtein', 'LowFat', \\\n",
    "                     'LowSugar', 'HighFiber']\n",
    "\n",
    "numeric_vars = get_variable_types(df)['Numeric']\n",
    "# remove original non-numeric variables \n",
    "for var in numeric_vars.copy():\n",
    "    if var in non_numeric_vars:\n",
    "        numeric_vars.remove(var)\n",
    "        \n",
    "print(\"Numeric vars: \", numeric_vars)\n",
    "\n",
    "# check for variables that are normally distributed\n",
    "norm_dist_variables = []\n",
    "for var in numeric_vars:\n",
    "    stat, p_value = shapiro(df[var])\n",
    "    # Interpret the result\n",
    "    alpha = 0.05\n",
    "    if p_value > alpha:\n",
    "        # The variable looks normally distributed (fail to reject H0)\n",
    "        norm_dist_variables.append(var)\n",
    "\n",
    "# print('Original dataset:', df.shape)\n",
    "# print('Normal distributed variables:', norm_dist_variables)\n",
    "# \"Rating\" is the only normally distributed variable but it only has 1 unique value in the dataset -> ignore it\n",
    "summary5 = df.describe(include='number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_outlier_thresholds(summary5: DataFrame, var: str, OPTION: str, OUTLIER_PARAM: int):\n",
    "    # default parameter\n",
    "    if OPTION == 'iqr':\n",
    "        iqr = OUTLIER_PARAM * (summary5[var]['75%'] - summary5[var]['25%'])\n",
    "        top_threshold = summary5[var]['75%'] + iqr\n",
    "        bottom_threshold = summary5[var]['25%'] - iqr\n",
    "    # for normal distribution\n",
    "    elif OPTION == 'stdev':\n",
    "        std = OUTLIER_PARAM * summary5[var]['std']\n",
    "        top_threshold = summary5[var]['mean'] + std\n",
    "        bottom_threshold = summary5[var]['mean'] - std\n",
    "    else:\n",
    "        raise ValueError('Unknown outlier parameter!')\n",
    "    return top_threshold, bottom_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 1**: Drop outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after dropping outliers: (82190, 29)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------- #\n",
    "# APPROACH 1: Drop outliers #\n",
    "# ------------------------- #\n",
    "\n",
    "# Tuned parameter to get the better results\n",
    "IQR_PARAM = 1.5\n",
    "\n",
    "data = df.copy(deep=True)\n",
    "\n",
    "for var in numeric_vars:\n",
    "    top_threshold, bottom_threshold = determine_outlier_thresholds(summary5, var, 'iqr', IQR_PARAM)\n",
    "    outliers = data[(data[var] > top_threshold) | (data[var] < bottom_threshold)]\n",
    "    # print(f'{var} outliers: {outliers.shape[0]}/{data[var].shape[0]}')\n",
    "    data.drop(outliers.index, axis=0, inplace=True)\n",
    "data.to_csv(f'datasets/outliers/{file_tag}_drop_outliers_{IQR_PARAM}.csv', index=True)\n",
    "print('Dataset after dropping outliers:', data.shape)\n",
    "\n",
    "# Best results: Training score = 0.557 ; Test score = 0.569 TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 2**: Truncate outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after truncating outliers: (140195, 29)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------- #\n",
    "# APPROACH 2: Truncate outliers #\n",
    "# ----------------------------- #\n",
    "\n",
    "# Tuned parameter to get the better results\n",
    "IQR_PARAM = 1.5\n",
    "\n",
    "data = df.copy(deep=True)\n",
    "\n",
    "for var in numeric_vars:\n",
    "    top_threshold, bottom_threshold = determine_outlier_thresholds(summary5, var, 'iqr', IQR_PARAM)\n",
    "    original_column = data[var].copy()\n",
    "    data[var] = data[var].apply(lambda x: top_threshold if x > top_threshold else bottom_threshold if x < bottom_threshold else x)\n",
    "    # print(f'{var} outliers: {(data[var] != original_column).sum()}/{data[var].shape[0]}')\n",
    "data.to_csv(f'datasets/outliers/{file_tag}_truncate_outliers_{IQR_PARAM}.csv', index=True)\n",
    "print('Dataset after truncating outliers:', data.shape)\n",
    "    \n",
    "# Best results: Training score = 0.557 ; Test score = 0.569 TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib.pyplot import figure, savefig, show, subplots\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "       \n",
    "# Best option: dataset_drop_outliers_1.5\n",
    "file_source_path = 'datasets/outliers/dataset_drop_outliers_1.5.csv' # source file\n",
    "file_dir = 'datasets/scaling' # destination directory\n",
    "file_tag = 'dataset'\n",
    "\n",
    "# read the data\n",
    "df = pd.read_csv(file_source_path, low_memory=False)\n",
    "convert_variable_types(df)\n",
    "index_column = df.columns[0]\n",
    "df = df.drop([index_column], axis=1)\n",
    "\n",
    "variable_types = get_variable_types(df)\n",
    "numeric_vars = variable_types['Numeric']\n",
    "categorical_vars = variable_types['Categorical']\n",
    "boolean_vars = variable_types['Binary']\n",
    "rest_vars = []\n",
    "\n",
    "# print('Numeric variables:', numeric_vars)\n",
    "# print('Categorical variables:', categorical_vars)\n",
    "# print('Boolean variables:', boolean_vars)\n",
    "\n",
    "non_numeric_vars =  ['TestSetId', 'AuthorId', 'Diet', 'Name', 'RecipeCategory', 'RecipeIngredientQuantities', \\\n",
    "                    'RecipeIngredientParts', 'RecipeYield', 'Like', 'HighCalories', 'HighProtein', 'LowFat', \\\n",
    "                    'LowSugar', 'HighFiber']\n",
    "\n",
    "# remove original non-numeric variables \n",
    "for var in numeric_vars.copy():\n",
    "    if var in non_numeric_vars:\n",
    "        numeric_vars.remove(var)\n",
    "        rest_vars.append(var)\n",
    "\n",
    "# Remove class (Like) from categorical variables\n",
    "categorical_vars.remove('Like')\n",
    "\n",
    "# print('Categorical variables:', categorical_vars)\n",
    "\n",
    "df_num = df[numeric_vars]\n",
    "df_symb = df[categorical_vars]\n",
    "df_bool = df[boolean_vars]\n",
    "df_rest = df[rest_vars]\n",
    "df_target = df['Like']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MinMax**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- #\n",
    "# MinMax normalization  #\n",
    "# --------------------- #\n",
    "\n",
    "transf = MinMaxScaler(feature_range=(0, 1), copy=True).fit(df_num)\n",
    "tmp = DataFrame(transf.transform(df_num), index=df.index, columns= numeric_vars)\n",
    "temp_norm_data_minmax = concat([tmp, df_rest, df_symb, df_bool], axis=1)\n",
    "norm_data_minmax = concat([temp_norm_data_minmax, df_target], axis=1)\n",
    "norm_data_minmax.to_csv(f'datasets/scaling/{file_tag}_scaled_minmax.csv', index=True)\n",
    "# print(norm_data_minmax.describe())\n",
    "\n",
    "# Best results: Training Score = 0.576 ; Test Score = 0.578"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Z-score / StandardScaler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- #\n",
    "# Z-score Normalization #\n",
    "# --------------------- #\n",
    "\n",
    "# scale numeric variables and concat to the rest to create a new csv file\n",
    "transf = StandardScaler(with_mean=True, with_std=True, copy=True).fit(df_num)\n",
    "tmp = DataFrame(transf.transform(df_num), index=df.index, columns= numeric_vars)\n",
    "temp_norm_data_zscore = concat([tmp, df_rest, df_symb, df_bool], axis=1)\n",
    "norm_data_zscore = concat([temp_norm_data_zscore, df_target], axis=1)\n",
    "norm_data_zscore.to_csv(f'datasets/scaling/{file_tag}_scaled_zscore.csv', index=True)\n",
    "# print(norm_data_zscore.describe())\n",
    "\n",
    "# NOTE: Results show that MinMax normalization is the best option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Best option: dataset_rating_drop_recipe_mean\n",
    "# file_source_path = 'datasets/scaling/dataset_dataset_drop_outliers_1.5_scaled_minmax.csv' # source file\n",
    "\n",
    "# # read the data\n",
    "# df = pd.read_csv(file_source_path, low_memory=False)\n",
    "# convert_variable_types(df)\n",
    "# index_column = df.columns[0]\n",
    "# df.drop([index_column], axis=1, inplace=True)\n",
    "\n",
    "# # We can also leverage the dataprep package to get a nice summary report\n",
    "# report = sv.analyze(df)\n",
    "# report.show_notebook()\n",
    "\n",
    "# # We can also leverage the yadata_profiling package to get a nice summary report\n",
    "# profile = ProfileReport(df, title=\"LLMeals - Summary Report\")\n",
    "# profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from matplotlib.pyplot import figure, title, savefig, show, tight_layout\n",
    "from seaborn import heatmap\n",
    "from matplotlib.pyplot import Axes\n",
    "\n",
    "# Best option: dataset_rating_drop_recipe_mean\n",
    "file_source_path = 'datasets/scaling/dataset_scaled_minmax.csv' # source file\n",
    "file_dir = 'datasets/outliers' # destination directory\n",
    "file_tag = 'dataset'\n",
    "\n",
    "# read the data\n",
    "df = pd.read_csv(file_source_path, low_memory=False)\n",
    "convert_variable_types(df)\n",
    "index_column = df.columns[0]\n",
    "df.drop([index_column], axis=1, inplace=True)\n",
    "\n",
    "# ---------------------------- #\n",
    "# Dropping Redundant Variables #\n",
    "# ---------------------------- #\n",
    "\n",
    "THRESHOLD = 0.8\n",
    "\n",
    "def select_redundant(corr_mtx, threshold: float) -> tuple[dict, DataFrame]:\n",
    "    if corr_mtx.empty:\n",
    "        return {}\n",
    "\n",
    "    corr_mtx = abs(corr_mtx)\n",
    "    vars_2drop = {}\n",
    "    for el in corr_mtx.columns:\n",
    "        el_corr = (corr_mtx[el]).loc[corr_mtx[el] >= threshold]\n",
    "        if len(el_corr) == 1:\n",
    "            corr_mtx.drop(labels=el, axis=1, inplace=True)\n",
    "            corr_mtx.drop(labels=el, axis=0, inplace=True)\n",
    "        else:\n",
    "            vars_2drop[el] = el_corr.index\n",
    "    return vars_2drop, corr_mtx\n",
    "\n",
    "drop, corr_mtx = select_redundant(df.corr(), THRESHOLD)\n",
    "# print(\"Redundancies: \", drop.keys())\n",
    "\n",
    "if corr_mtx.empty:\n",
    "    raise ValueError('Matrix is empty.')\n",
    "\n",
    "# figure(figsize=[10, 10])\n",
    "# heatmap(corr_mtx, xticklabels=corr_mtx.columns, yticklabels=corr_mtx.columns, annot=False, cmap='Blues')\n",
    "# title('Filtered Correlation Analysis')\n",
    "# tight_layout()\n",
    "# show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables before drop:  dict_keys(['Time', 'CookTime', 'Calories', 'FatContent', 'SaturatedFatContent'])\n",
      "Variables to drop:  ['CookTime', 'FatContent']\n"
     ]
    }
   ],
   "source": [
    "# AUX: Drop redundant variables\n",
    "def drop_redundant(data: DataFrame, vars_2drop: dict) -> DataFrame:\n",
    "    sel_2drop = []\n",
    "    for key in vars_2drop.keys():\n",
    "        if key not in sel_2drop:\n",
    "            for r in vars_2drop[key]:\n",
    "                if r != key and r not in sel_2drop:\n",
    "                    sel_2drop.append(r)\n",
    "    print('Variables to drop: ', sel_2drop)\n",
    "    df = data.copy()\n",
    "    for var in sel_2drop:\n",
    "        df.drop(labels=var, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "print(\"Variables before drop: \", drop.keys())\n",
    "df = drop_redundant(df, drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No variables to drop.\n"
     ]
    }
   ],
   "source": [
    "# AUX: plot bar chart\n",
    "def bar_chart_fs(xvalues: list, yvalues: list, ax: Axes = None, title: str = '', xlabel: str = '', ylabel: str = '', percentage: bool = False, rotation: bool = False):\n",
    "    ax = set_elements(ax=ax, title=title, xlabel=xlabel, ylabel=ylabel, percentage=percentage)\n",
    "    set_locators(xvalues, ax=ax, rotation=rotation)\n",
    "    ax.bar(xvalues, yvalues, edgecolor=cfg.LINE_COLOR, color=cfg.FILL_COLOR, tick_label=xvalues)\n",
    "    for i in range(len(yvalues)):\n",
    "        ax.text(i, yvalues[i] + TEXT_MARGIN/120, f'{yvalues[i]:.2f}', ha='center', fontproperties=FONT_TEXT)\n",
    "\n",
    "# AUX: select low variance variables\n",
    "def select_low_variance(data: DataFrame, threshold: float) -> list:\n",
    "    lst_variables = []\n",
    "    lst_variances = []\n",
    "    for el in data.columns:\n",
    "        value = data[el].var()\n",
    "        if value <= threshold:\n",
    "            lst_variables.append(el)\n",
    "            lst_variances.append(value)\n",
    "\n",
    "    # print(len(lst_variables), lst_variables)\n",
    "    if len(lst_variables) == 0:\n",
    "        return []\n",
    "    else:\n",
    "        figure(figsize=[16, 10])\n",
    "        bar_chart_fs(lst_variables, lst_variances, title='Variance analysis', xlabel='variables', ylabel='variance', rotation=True)\n",
    "        tight_layout()\n",
    "        show()\n",
    "        return lst_variables\n",
    "\n",
    "# ----------------------------------------------- #\n",
    "\n",
    "numeric = get_variable_types(df)['Numeric']\n",
    "vars_2drop = select_low_variance(data[numeric], 0.15)\n",
    "\n",
    "if len(vars_2drop) > 0:\n",
    "    df = drop_redundant(df, vars_2drop)\n",
    "    print('Variables to drop: ', vars_2drop)\n",
    "else:\n",
    "    print('No variables to drop.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()\n",
    "df.to_csv(f'datasets/feature_selection/{file_tag}_selected.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a cleaned data set, and before we start the *Modelling* phase, we are going to split our data set into multiple sub-datasets. \n",
    "Here, we are going to balance the data to ensure that both classes are equally represented, and then split it into an *train* and *test* data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into learning and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "file_source_path = 'datasets/feature_selection/dataset_selected.csv' # source file\n",
    "target = 'Like'\n",
    "\n",
    "# read data\n",
    "df = pd.read_csv(f'{file_source_path}', low_memory=False)\n",
    "# convert variable types\n",
    "convert_variable_types(df)\n",
    "# remove index column\n",
    "index_column = df.columns[0]\n",
    "df = df.drop([index_column], axis=1)\n",
    "# Drop TestSetId column and Like NaN rows (no way to know if they liked or not)\n",
    "df.drop('TestSetId', axis=1, inplace=True)\n",
    "df.dropna(subset=[target], inplace=True)\n",
    "\n",
    "# ----------------------------- #\n",
    "#           BALANCING           #\n",
    "# ----------------------------- #\n",
    "\n",
    "like_zero = df[df[target] == 0.0]\n",
    "like_one = df[df[target] == 1.0]\n",
    "\n",
    "df_one_sample = like_one.sample(len(like_zero), replace=True)\n",
    "df_zero_sample = like_zero.sample(len(like_zero))\n",
    "\n",
    "df = pd.concat([df_zero_sample, df_one_sample], axis=0)\n",
    "\n",
    "# print(df.info())\n",
    "# print(df[target].value_counts())\n",
    "\n",
    "y = df.pop(target).values\n",
    "X = df.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y,\n",
    "                test_size=0.3, \n",
    "                shuffle=True,\n",
    "                random_state=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'> Phase 4: Modeling </font>\n",
    "\n",
    "In this phase, the model is trained and tuned. In general, data transformations\n",
    "from data wrangling can be part of a machine learning pipeline, and can therefore\n",
    "be tuned as well. (See CRISP-DM: DataPrep <--> Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter: {'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 25, 'model__n_estimators': 75, 'pca__n_components': 25} (CV score=0.964)\n"
     ]
    }
   ],
   "source": [
    "# Here, we want to find the best classifier. As candidates, we consider\n",
    "#   1. LogisticRegression\n",
    "#   2. RandomForestClassifier\n",
    "#   3. GradientBoostingClassifier\n",
    "#   4. MLPClassifier\n",
    "    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model_logistic_regression = LogisticRegression(max_iter=300)\n",
    "model_random_forest = RandomForestClassifier()\n",
    "model_gradient_boosting = GradientBoostingClassifier()\n",
    "model_neural_network = MLPClassifier(max_iter=200)\n",
    "\n",
    "# # train the models\n",
    "pipeline = Pipeline(steps=[(\"pca\", PCA()),\n",
    "                           (\"model\", None)])\n",
    "\n",
    "parameter_grid_preprocessing = {\n",
    "  # \"pca__n_components\" : [1, 2, 3, 4],\n",
    "  \"pca__n_components\" : [df.shape[1]-12, df.shape[1]-8, df.shape[1]-5, df.shape[1]]\n",
    "}\n",
    "\n",
    "# NOTE: Logistic Regression does not perform as well as the other models\n",
    "parameter_grid_logistic_regression = {\n",
    "  \"model\" : [model_logistic_regression],\n",
    "  \"model__C\" : [0.1, 1, 10],  # inverse regularization strength\n",
    "}\n",
    "\n",
    "parameter_grid_gradient_boosting = {\n",
    "  \"model\" : [model_gradient_boosting],\n",
    "  \"model__n_estimators\" : [20, 30, 50]\n",
    "}\n",
    "\n",
    "parameter_grid_random_forest = {\n",
    "  \"model\" : [model_random_forest],\n",
    "  \"model__n_estimators\" : [30, 50, 75],  # number of max trees in the forest\n",
    "  \"model__max_depth\" : [20, df.shape[1]],\n",
    "}\n",
    "\n",
    "# NOTE: NN does not perform well on this dataset + takes a long time to train\n",
    "parameter_grid_neural_network = {\n",
    "    \"model\": [model_neural_network],\n",
    "    \"model__hidden_layer_sizes\": [(30, 30), (40, 30)],  # Example hidden layer configurations\n",
    "    \"model__alpha\": [0.0001],  # Regularization parameter\n",
    "}\n",
    "\n",
    "meta_parameter_grid = [\n",
    "                      # parameter_grid_logistic_regression,\n",
    "                       parameter_grid_random_forest,\n",
    "                       parameter_grid_gradient_boosting] #,\n",
    "                      #  parameter_grid_neural_network]\n",
    "\n",
    "meta_parameter_grid = [{**parameter_grid_preprocessing, **model_grid}\n",
    "                       for model_grid in meta_parameter_grid]\n",
    "\n",
    "search = GridSearchCV(pipeline,\n",
    "                      meta_parameter_grid, \n",
    "                      scoring=\"balanced_accuracy\",\n",
    "                      n_jobs=-1, \n",
    "                      cv=5,  # number of folds for cross-validation \n",
    "                      error_score=\"raise\"\n",
    ")\n",
    "\n",
    "# here, the actual training and grid search happens\n",
    "search.fit(X_train, y_train.ravel())\n",
    "\n",
    "print(\"best parameter:\", search.best_params_ ,\"(CV score=%0.3f)\" % search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'> Step 5: Evaluation </font>\n",
    "\n",
    "Once the appropriate models are chosen, they are evaluated on the test set. For\n",
    "this, different evaluation metrics can be used. Furthermore, this step is where\n",
    "the models and their predictions are analyzed resp. different properties, including\n",
    "feature importance, robustness to outliers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on test set: 0.9703168829482585\n",
      "true    0.0    1.0\n",
      "pred              \n",
      "0.0   14370     28\n",
      "1.0     877  15134\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance of model on test set\n",
    "print(\"Score on test set:\", search.score(X_test, y_test.ravel()))\n",
    "\n",
    "# contingency table\n",
    "ct = pd.crosstab(search.best_estimator_.predict(X_test), y_test.ravel(),\n",
    "                 rownames=[\"pred\"], colnames=[\"true\"])\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 20, 'model__n_estimators': 30, 'pca__n_components': 13} 0.9475653113555316\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 20, 'model__n_estimators': 30, 'pca__n_components': 17} 0.9435763592696901\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 20, 'model__n_estimators': 30, 'pca__n_components': 20} 0.9468614698702142\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 20, 'model__n_estimators': 30, 'pca__n_components': 25} 0.9514482228978268\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 20, 'model__n_estimators': 50, 'pca__n_components': 13} 0.9484550161467444\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 20, 'model__n_estimators': 50, 'pca__n_components': 17} 0.9455458307268237\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 20, 'model__n_estimators': 50, 'pca__n_components': 20} 0.9473282851070357\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 20, 'model__n_estimators': 50, 'pca__n_components': 25} 0.9523741249225119\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 20, 'model__n_estimators': 75, 'pca__n_components': 13} 0.9498718783807341\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 20, 'model__n_estimators': 75, 'pca__n_components': 17} 0.945178489381529\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 20, 'model__n_estimators': 75, 'pca__n_components': 20} 0.9488346437612443\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 20, 'model__n_estimators': 75, 'pca__n_components': 25} 0.9531904695574525\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 25, 'model__n_estimators': 30, 'pca__n_components': 13} 0.95865967344086\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 25, 'model__n_estimators': 30, 'pca__n_components': 17} 0.9558698633634757\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 25, 'model__n_estimators': 30, 'pca__n_components': 20} 0.958009622528073\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 25, 'model__n_estimators': 30, 'pca__n_components': 25} 0.9625860744449415\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 25, 'model__n_estimators': 50, 'pca__n_components': 13} 0.960685188236426\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 25, 'model__n_estimators': 50, 'pca__n_components': 17} 0.9566574995421238\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 25, 'model__n_estimators': 50, 'pca__n_components': 20} 0.9593690922599867\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 25, 'model__n_estimators': 50, 'pca__n_components': 25} 0.9630431683485376\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 25, 'model__n_estimators': 75, 'pca__n_components': 13} 0.9596103390334114\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 25, 'model__n_estimators': 75, 'pca__n_components': 17} 0.9564443715358397\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 25, 'model__n_estimators': 75, 'pca__n_components': 20} 0.9598890796625875\n",
      "{'model': RandomForestClassifier(max_depth=25, n_estimators=75), 'model__max_depth': 25, 'model__n_estimators': 75, 'pca__n_components': 25} 0.9638030807917717\n",
      "{'model': GradientBoostingClassifier(), 'model__n_estimators': 20, 'pca__n_components': 13} 0.6906969363864522\n",
      "{'model': GradientBoostingClassifier(), 'model__n_estimators': 20, 'pca__n_components': 17} 0.74032645851154\n",
      "{'model': GradientBoostingClassifier(), 'model__n_estimators': 20, 'pca__n_components': 20} 0.7403545271266472\n",
      "{'model': GradientBoostingClassifier(), 'model__n_estimators': 20, 'pca__n_components': 25} 0.7391076114609899\n",
      "{'model': GradientBoostingClassifier(), 'model__n_estimators': 30, 'pca__n_components': 13} 0.702126805305792\n",
      "{'model': GradientBoostingClassifier(), 'model__n_estimators': 30, 'pca__n_components': 17} 0.7493733497390342\n",
      "{'model': GradientBoostingClassifier(), 'model__n_estimators': 30, 'pca__n_components': 20} 0.7493845807375206\n",
      "{'model': GradientBoostingClassifier(), 'model__n_estimators': 30, 'pca__n_components': 25} 0.7505131861469503\n",
      "{'model': GradientBoostingClassifier(), 'model__n_estimators': 50, 'pca__n_components': 13} 0.7153905769539607\n",
      "{'model': GradientBoostingClassifier(), 'model__n_estimators': 50, 'pca__n_components': 17} 0.7635929059037505\n",
      "{'model': GradientBoostingClassifier(), 'model__n_estimators': 50, 'pca__n_components': 20} 0.7632890668082\n",
      "{'model': GradientBoostingClassifier(), 'model__n_estimators': 50, 'pca__n_components': 25} 0.7645628448455366\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# (optional, if you're curious) \n",
    "# for a detailed look on the performance of the different models\n",
    "def get_search_score_overview():\n",
    "  for c,s in zip(search.cv_results_[\"params\"],search.cv_results_[\"mean_test_score\"]):\n",
    "      print(c, s)\n",
    "\n",
    "print(get_search_score_overview())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretability\n",
    "\n",
    "##### Disclaimer: This only works if shap is installed.\n",
    "\n",
    "In addition to models and their predictions, it is often important to understand _why_ a model makes certain predictions. \n",
    "There is a lot of literature on how this can be achieved (explainability), but we will only show the use of Shapley values\n",
    "using the python module \"shap\", which is a combination of Shapley values and LIME. \n",
    "You can find more information on this topic [here](https://christophm.github.io/interpretable-ml-book/shap.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # assume random forest model\n",
    "# model = RandomForestClassifier(n_estimators=10, random_state=seed)\n",
    "# model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# # compute shapley values\n",
    "# explainer = shap.TreeExplainer(model)\n",
    "# shap_values = explainer.shap_values(X_train)\n",
    "# shap_interaction_values = explainer.shap_interaction_values(X_train)\n",
    "\n",
    "# expected_value = explainer.expected_value\n",
    "# print(expected_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # class dependent plots of shapley values for each feature\n",
    "# for i,c in enumerate(df.variety.unique()):\n",
    "#     shap.summary_plot(shap_values[i], X_train, show=False)\n",
    "#     plt.title(\"Shapley values for \"+str(c))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the computed SHAP values, we can interpret that the *petal.width* has a positive impact on the output of the model \n",
    "if the feature value is moderate. For high aand low values, the impact is negative. The same observation\n",
    "holds for *petal.length*. Besides, the impact of the *sepal.length* and *sepal.width* features are rather low. By impact on a \n",
    "the target, we model the probability that we classify that target. Thus, if *petal.width* is high, it is more likely\n",
    "that we classify the data point as Versicolor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'> Step 6: Deployment </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_service_classify_review(datapoint):\n",
    "  # make sure the provided datapoints adhere to the correct format for model input\n",
    "  # 'TestSetId' is not a feature used for prediction\n",
    "  datapoint = datapoint.drop('TestSetId', axis=1)\n",
    "  # fetch your trained model\n",
    "  model = search.best_estimator_\n",
    "\n",
    "  # make prediction with the model\n",
    "  prediction = model.predict(datapoint)\n",
    "\n",
    "  return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Analytics Cup, we need to export your prediction in a very specific output format. This is a csv file without an index and two columns, *id* and *prediction*. Note that the values in both columns need to be integer values, and especially in the *prediction* column either 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X has feature names, but PCA was fitted without feature names\n"
     ]
    }
   ],
   "source": [
    "file_source_path = 'datasets/feature_selection/dataset_selected.csv' # source file\n",
    "\n",
    "# read data\n",
    "df = pd.read_csv(f'{file_source_path}', low_memory=False)\n",
    "# convert variable types\n",
    "convert_variable_types(df)\n",
    "# remove index column\n",
    "index_column = df.columns[0]\n",
    "df = df.drop([index_column], axis=1)\n",
    "\n",
    "# keep only the rows without a Like value\n",
    "df = df[df['Like'].isna()]\n",
    "# remove the Like column\n",
    "df.drop('Like', axis=1, inplace=True)\n",
    "\n",
    "# make the missing predictions for the Like column\n",
    "df['Like'] = micro_service_classify_review(df)\n",
    "\n",
    "# create a dataset that contains only the column \n",
    "# with the TestSetId and the model prediction for Like\n",
    "output = df[['TestSetId', 'Like']]\n",
    "\n",
    "# rename the columns to match the required format\n",
    "output = output.rename(columns={'TestSetId': 'id'})\n",
    "output = output.rename(columns={'Like': 'prediction'})\n",
    "submission = output.reindex(columns=[\"id\", \"prediction\"])\n",
    "# convert id and prediction to integer\n",
    "submission['id'] = submission['id'].astype(int)\n",
    "submission['prediction'] = submission['prediction'].astype(int)\n",
    "\n",
    "# print(submission.head())\n",
    "\n",
    "# save the predictions to a CSV file\n",
    "submission.to_csv('predictions_tugas.csv', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
