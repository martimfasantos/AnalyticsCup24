{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"font-weight: bold;\"> Analytics Cup 2024 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Note: The following do not work with Python 3.12\n",
    "import shap\n",
    "from ydata_profiling import ProfileReport\n",
    "import sweetviz as sv\n",
    "\n",
    "import os\n",
    "from matplotlib.pyplot import figure, title, savefig, show, tight_layout, subplots\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from pandas import concat, DataFrame\n",
    "from sklearn.impute import SimpleImputer\n",
    "from numpy import nan\n",
    "from fractions import Fraction\n",
    "from seaborn import heatmap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2024\n",
    "\n",
    "# pandas, statsmodels, matplotlib and y_data_profiling rely on numpy's random generator, and thus, we need to set the seed in numpy\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'> Phase 1: Business Understanding </font>\n",
    "\n",
    "Business Understanding is the first and economically most important step in the\n",
    "CRISP-DM process. It serves to assess use cases, feasibility, requirements, and\n",
    "risks of the endeavored data driven project. Since the conduction of data driven\n",
    "projects usually depends on the data at hand, the CRISP-DM process often \n",
    "alternates between Business Understanding and Data Understanding, until the\n",
    "project's schedule becomes sufficiently clear.\n",
    "\n",
    "#### Business Understanding\n",
    "\n",
    "In LLMeals Analytics Cup, the goal is to improve customer satisfaction and reduce subscription cancellations by developing a model that accurately predicts whether a customer likes (Like=1) or dislikes (Like=0) a suggested recipe. The model will utilize datasets such as \"recipes.csv,\" \"reviews.csv,\" \"diet.csv,\" and \"requests.csv\" to generate insights into user preferences. The successful model will serve as the foundation for enhancing the quality of suggested recipes in LLMeals' service, aligning them more closely with individual customer requirements. The project's ultimate aim is to leverage data-driven approaches for refining the recipe suggestions and, in turn, improving the overall LLMeals user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'> Phase 2: Data Understanding </font>\n",
    "\n",
    "The *Data Understanding* phase mainly serves to inform the Business Understanding step by\n",
    "assessing the data quality and content, and should provide the engineers with \n",
    "an intuition for the specific data and the specific problem at hand. Experienced\n",
    "data scientists and machine learning engineers can often estimate the difficulty\n",
    "and feasibility of the task by analyzing and understanding the data.  \n",
    "\n",
    "#### Example: Data Understanding\n",
    "\n",
    "Make yourself familiar with the structure and content of the data. *Note*, this step \n",
    "heavily depends on the specific problem at hand, since there is no fixed recipe that \n",
    "fits all possible data sets. In the example below, we are only looking at a very small\n",
    "data set and do **not** conduct an in-depth analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "file_dir = \".\"\n",
    "file_names = [\"reviews.csv\", \"requests.csv\", \"diet.csv\", \"recipes.csv\"]\n",
    "reviews = pd.read_csv(f'{file_dir}/{file_names[0]}', low_memory=False)\n",
    "requests = pd.read_csv(f'{file_dir}/{file_names[1]}')\n",
    "diet = pd.read_csv(f'{file_dir}/{file_names[2]}')\n",
    "recipes = pd.read_csv(f'{file_dir}/{file_names[3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(reviews.sample(3))\n",
    "# print(\"\\n\")\n",
    "# print(reviews.info())\n",
    "# print(\"\\n\")\n",
    "# print(reviews.describe())\n",
    "# sns.boxplot(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(requests.sample(3))\n",
    "# print(\"\\n\")\n",
    "# print(requests.info())\n",
    "# print(\"\\n\")\n",
    "# print(requests.describe())\n",
    "# sns.boxplot(requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(diet.sample(3))\n",
    "# print(\"\\n\")\n",
    "# print(diet.info())\n",
    "# print(\"\\n\")\n",
    "# print(diet.describe())\n",
    "# sns.boxplot(diet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(recipes.sample(3))\n",
    "# print(\"\\n\")\n",
    "# print(recipes.info())\n",
    "# print(\"\\n\")\n",
    "# print(recipes.describe())\n",
    "# plt.figure(figsize=(24, 6))\n",
    "# sns.boxplot(recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check the balancing of classes/labels\n",
    "# print(reviews.groupby(\"Like\").size())\n",
    "\n",
    "# # -> 2 classes, 1 is much more frequent than the other (False/True ratio is ~4:1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviews Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the feature distributions with a pairplot,\n",
    "# as it gives you a good overview over possible outliers\n",
    "# and a good overview over the data in general\n",
    "\n",
    "# pairplot for the full data\n",
    "# columns_to_drop = [\"AuthorId\", \"RecipeId\", \"TestSetId\"]\n",
    "# sns.pairplot(reviews.drop(columns_to_drop, axis=1), hue=\"Like\", diag_kind=\"hist\", diag_kws={\"multiple\" : \"stack\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requests Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the feature distributions with a pairplot,\n",
    "# as it gives you a good overview over possible outliers\n",
    "# and a good overview over the data in general\n",
    "\n",
    "# pairplot for the full data\n",
    "# columns_to_drop = [\"AuthorId\", \"RecipeId\"]\n",
    "# data = pd.merge(requests, reviews[[\"AuthorId\", \"RecipeId\", \"Like\"]], on=['AuthorId','RecipeId'], \\\n",
    "#     how='left').drop(columns_to_drop, axis=1)\n",
    "# sns.pairplot(data, hue=\"Like\", diag_kind=\"hist\", diag_kws={\"multiple\" : \"stack\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diet Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairplot for the full data\n",
    "# columns_to_drop = [\"AuthorId\"]\n",
    "# data = pd.merge(diet, reviews[[\"AuthorId\", \"Like\"]], on=['AuthorId'], how='left').drop(columns_to_drop, axis=1)\n",
    "# sns.pairplot(data, hue=\"Like\", diag_kind=\"hist\", diag_kws={\"multiple\" : \"stack\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recipes Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairplot for the full data\n",
    "# columns_to_drop = [\"RecipeId\"]\n",
    "# data = pd.merge(recipes, reviews[[\"RecipeId\", \"Like\"]], on=['RecipeId'], how='left').drop(columns_to_drop, axis=1)\n",
    "# sns.pairplot(data, hue=\"Like\", diag_kind=\"hist\", diag_kws={\"multiple\" : \"stack\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a merged dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **File**     | **Join Keys**            |\n",
    "|--------------|------------------------|\n",
    "| reviews.csv  | AuthorId, RecipeId     |\n",
    "| requests.csv | AuthorId, RecipeId     |\n",
    "| diet.csv     | AuthorId               |\n",
    "| recipes.csv  | RecipeId               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes using multiple columns\n",
    "merged_df = pd.merge(reviews, requests, on=['AuthorId','RecipeId'], how='left')\n",
    "merged_df = pd.merge(merged_df, diet, on='AuthorId', how='left')\n",
    "merged_df = pd.merge(merged_df, recipes, on='RecipeId', how='left')\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv('merged_data.csv', index=True)\n",
    "\n",
    "# Check if the number of rows and columns are correct\n",
    "# print(len(merged_df) == len(reviews))\n",
    "# print(reviews.shape)\n",
    "# print(merged_df.shape)\n",
    "# print(merged_df.shape[1] == reviews.shape[1] + requests.shape[1]-2 + diet.shape[1]-1 + recipes.shape[1]-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "file_dir = \".\"\n",
    "file_name = \"merged_data.csv\"\n",
    "df = pd.read_csv(f'{file_dir}/{file_name}', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class-dependent pairplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We can also leverage the dataprep package to get a nice summary report\n",
    "# report = sv.analyze(df)\n",
    "# report.show_notebook()\n",
    "\n",
    "# # We can also leverage the yadata_profiling package to get a nice summary report\n",
    "# profile = ProfileReport(df, title=\"LLMeals - Summary Report\")\n",
    "# profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: Data Understanding\n",
    "\n",
    "You should have a good understanding what the data is about and of some of its properties. Newly gained insights are used to reiterate the\n",
    "Business Understanding Phase, but in this example, it won't be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'> Phase 3: Data Preparation </font>\n",
    "\n",
    "Data Preparation mainly consists of two parts, Data Cleaning and Data Wrangling. In Data\n",
    "Cleaning, the goal is assure data quality. This includes removing wrong/corrupt \n",
    "data entries and making sure the entries are standardized, e.g. enforcing certain encodings. \n",
    "Data Wrangling then transforms the data in order to make it suitable for the modelling step.\n",
    "Sometimes, steps from Data Wrangling are incorporated into the automatized Pipeline, as\n",
    "we will show in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding variables...\n"
     ]
    }
   ],
   "source": [
    "file_source_path = 'merged_data.csv' # source file\n",
    "file_dir = '.' # destination directory\n",
    "file_tag = 'dataset'\n",
    "\n",
    "df = pd.read_csv(file_source_path, low_memory=False)\n",
    "index_column = df.columns[0]\n",
    "df.drop([index_column], axis=1, inplace=True)\n",
    "\n",
    "# ----------- Convertions ----------- #\n",
    "# Like: object -> bool\n",
    "# HighProtein: {Indiferent, Yes} - object -> bool\n",
    "# LowSugar: {0, Indiferent} - object -> bool\n",
    "# Diet: {Vegetarian, Omnivore, Vegan} - object -> categorical\n",
    "# Name: 140k values, 50% distinct - object -> categorical\n",
    "# RecipeCategory: 7 categories - object -> categorical\n",
    "# RecipeIngredientQuantities: filter\n",
    "# RecipeIngredientParts: filter\n",
    "# RecipeYield: 46k values, 7.9k distinct, 93.8k missing - object -> categorical\n",
    "\n",
    "# AUX: dummify the variables\n",
    "def dummify_var(df, var_to_dummify):\n",
    "    other_vars = [c for c in df.columns if not c in var_to_dummify]\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', sparse=False, dtype=int)\n",
    "    X = df[var_to_dummify]\n",
    "    encoder.fit(X)\n",
    "    new_vars = encoder.get_feature_names_out(var_to_dummify)\n",
    "    trans_X = encoder.transform(X)\n",
    "    dummy = pd.DataFrame(trans_X, columns=new_vars, index=X.index)\n",
    "    #dummy = dummy.convert_dtypes(convert_boolean=True)\n",
    "\n",
    "    final_df = pd.concat([df[other_vars], dummy], axis=1)\n",
    "    return final_df\n",
    "\n",
    "# AUX: sum the numeric values\n",
    "def sum_numeric_values(strings):\n",
    "    cleaned_numbers = parse_ingredients(strings)    \n",
    "    total_sum = 0\n",
    "    try:\n",
    "        for num in cleaned_numbers:\n",
    "            nums = num.split('-')\n",
    "            nums = [n.strip() for n in nums]\n",
    "            quantities = []\n",
    "            for n in nums:\n",
    "                parts = n.split(' ')\n",
    "                if len(parts) == 2:\n",
    "                    # Handle mixed numbers\n",
    "                    quantities.append(float(Fraction(parts[0])) + float(Fraction(parts[1])))\n",
    "                    # print(f\"{float(Fraction(parts[0]))} {float(Fraction(parts[1]))}\")\n",
    "                elif '/' in n:\n",
    "                    # Handle fractions\n",
    "                    quantities.append(float(Fraction(n)))\n",
    "                    # print(Fraction(n))\n",
    "                else:\n",
    "                    # Handle regular floats\n",
    "                    quantities.append(float(n))\n",
    "                    # print(float(n))\n",
    "            total_sum += np.mean(quantities)\n",
    "    # Handle wrong format of data\n",
    "    except:\n",
    "        return 0\n",
    "        \n",
    "    return total_sum\n",
    "\n",
    "\n",
    "# AUX: parse the ingredient parts\n",
    "def parse_ingredients(ingredient_parts):    \n",
    "    # Clean the ingredient parts\n",
    "    cleaned_string = ingredient_parts.replace('c(', '').replace('\"', '').replace('\\\\', '').replace(')', '').split(',')\n",
    "    cleaned_ingredients = [ingredient.strip() for ingredient in cleaned_string]\n",
    "    for ing in cleaned_ingredients:\n",
    "        if 'character' in ing:\n",
    "            cleaned_ingredients.remove(ing)\n",
    "    return cleaned_ingredients\n",
    "\n",
    "meat_keywords = ['chicken', 'beef', 'pork', 'shrimp', 'salmon', 'sausage', 'bacon', 'turkey', 'ham', 'lamb', \\\n",
    "    'fish', 'meatballs', 'steak', 'tuna', 'ground beef', 'venison', 'duck', 'lobster', 'crab', 'oysters', 'clams', \\\n",
    "    'mussels', 'scallops', 'squid', 'octopus', 'escargot', 'prawns', 'crawfish', 'bison', 'rabbit', 'quail', 'goose', \\\n",
    "    'foie gras', 'veal', 'liver', 'tripe', 'anchovies', 'sardines', 'haddock', 'halibut', 'catfish', 'swordfish', 'hamburger patties', \\\n",
    "    'hot dogs', 'corned beef', 'lamb chops', 'liverwurst', 'chorizo', 'pepperoni', 'salami', 'pastrami', 'prosciutto', 'pate']\n",
    "vegetarian_keywords = ['butter', 'eggs', 'milk', 'cheese', 'cream', 'honey', 'mayonnaise', 'cheddar', \\\n",
    "    'cream', 'margarine', 'mustard', 'buttermilk', 'mozzarella', 'oil', 'jack', 'feta', 'yogurt', 'cheddar', \\\n",
    "    'cottage', 'provolone', 'gruyere', 'brie', 'romano', 'goat', 'ricotta', 'asiago', 'fontina', 'colby',  \\\n",
    "    'gouda', 'fraiche', 'gelatin']\n",
    "\n",
    "def create_products_columns(row):\n",
    "    name_keywords_meat = [keyword for keyword in meat_keywords if keyword in row['Name'].lower()]\n",
    "    name_keywords_vegetarian = [keyword for keyword in vegetarian_keywords if keyword in row['Name'].lower()]\n",
    "    \n",
    "    recipe_keywords_meat = [keyword for keyword in meat_keywords if keyword in parse_ingredients(row['RecipeIngredientParts'].lower())]\n",
    "    recipe_keywords_vegetarian = [keyword for keyword in vegetarian_keywords if keyword in parse_ingredients(row['RecipeIngredientParts'].lower())]\n",
    "\n",
    "    contains_meat = bool(name_keywords_meat) or bool(recipe_keywords_meat)\n",
    "    contains_non_vegan = bool(name_keywords_vegetarian) or bool(recipe_keywords_vegetarian)\n",
    "\n",
    "    return pd.Series([int(contains_meat), int(not contains_meat and contains_non_vegan), int(not contains_non_vegan)])\n",
    "\n",
    "    \n",
    "# ----------------------------------------------------------- #\n",
    "# IMPORTANT: function to encode the variables for the dataset #\n",
    "# ----------------------------------------------------------- #\n",
    "def encode_variables(df):\n",
    "    # Like: object -> bool\n",
    "    like_mapping = {False: 0, True: 1}\n",
    "    df[\"Like\"] = df[\"Like\"].replace(like_mapping).astype('category')\n",
    "    \n",
    "    # HighProtein: {Indiferent, Yes} - object -> categorical\n",
    "    hp_mapping = {'Indifferent': 0, 'Yes': 1}\n",
    "    df[\"HighProtein\"] = df[\"HighProtein\"].replace(hp_mapping)\n",
    "    df[\"HighProtein\"] = df[\"HighProtein\"].astype('category')\n",
    "    \n",
    "    # LowSugar: {0, Indiferent} - object -> categorical\n",
    "    ls_mapping = {'0': 0, 'Indifferent': 1}\n",
    "    df[\"LowSugar\"] = df[\"LowSugar\"].replace(ls_mapping)\n",
    "    df[\"LowSugar\"] = df[\"LowSugar\"].astype('category')\n",
    "    \n",
    "    # Diet: {Vegetarian, Omnivore, Vegan} - object -> categorical\n",
    "    df = dummify_var(df, [\"Diet\"])\n",
    "    \n",
    "    # Name: 140k values, 50% distinct - object -> categorical\n",
    "    df[['MeatMeal', 'VegetarianMeal', 'VeganMeal']] = df.apply(create_products_columns, axis=1)\n",
    "    # Drop the Name column\n",
    "    df.drop([\"Name\"], axis=1, inplace=True)\n",
    "    \n",
    "    # RecipeCategory: 7 categories - object -> categorical\n",
    "    df = dummify_var(df, [\"RecipeCategory\"])\n",
    "    \n",
    "    # RecipeYield: 46k values, 7.9k distinct, 93.8k missing - object -> categorical\n",
    "    df.drop([\"RecipeYield\"], axis=1, inplace=True)\n",
    "    \n",
    "    # RecipeIngredientQuantities: compute sum and drop\n",
    "    df[\"RecipeIngredientQuantitiesTotal\"] = df[\"RecipeIngredientQuantities\"].apply(sum_numeric_values)\n",
    "    df[\"NumberOfRecipeIngredients\"] = df[\"RecipeIngredientQuantities\"].apply(lambda x: len(parse_ingredients(x)))\n",
    "    df.drop([\"RecipeIngredientQuantities\"], axis=1, inplace=True)\n",
    "    \n",
    "    # Number of ingredients match\n",
    "    df[\"MatchNumberOfIngredients\"] = df.apply(lambda x: 1 if x[\"NumberOfRecipeIngredients\"] == len(parse_ingredients(x[\"RecipeIngredientParts\"])) else 0, axis=1)\n",
    "    \n",
    "    # RecipeIngredientParts: drop\n",
    "    df.drop([\"RecipeIngredientParts\"], axis=1, inplace=True)\n",
    "    \n",
    "    # Drop ID columns \n",
    "    # AuthorId: 140k values, 35% distinct\n",
    "    df.drop([\"AuthorId\"], axis=1, inplace=True)\n",
    "    df.drop([\"RecipeId\"], axis=1, inplace=True)\n",
    "    # Note: \"Rating\" was kept as float64 to allow for decimal values\n",
    "    \n",
    "    # New column: TotalTime\n",
    "    df['TotalTime'] = df['CookTime'] + df['PrepTime']\n",
    "    # New column: RespectsRequestedTime + drop Time\n",
    "    df['RespectsRequestedTime'] = df.apply(lambda x: 1 if x['TotalTime'] <= x['Time'] else 0, axis=1)\n",
    "    df['TimeDifference'] = df.apply(lambda x: x['TotalTime'] - x['Time'], axis=1)\n",
    "    df.drop([\"Time\"], axis=1, inplace=True)\n",
    "    \n",
    "    # New colums for Age: <30, 30-60, >60\n",
    "    #      Like         False     True \n",
    "    #      Age                               \n",
    "    # (17.999, 30.0]  0.977451  0.022549\n",
    "    # (30.0, 60.0]    0.884422  0.115578\n",
    "    # (60.0, 79.0]    0.751216  0.248784\n",
    "    df['Age_30'] = df['Age'].apply(lambda x: 1 if x < 30 else 0)\n",
    "    df['Age_30_60'] = df['Age'].apply(lambda x: 1 if x >= 30 and x <= 60 else 0)\n",
    "    df['Age_60'] = df['Age'].apply(lambda x: 1 if x > 60 else 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# encode the variables\n",
    "print('Encoding variables...')\n",
    "df = encode_variables(df)\n",
    "\n",
    "df.to_csv(f'{file_dir}/{file_tag}_encoded.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUX: Get variable types\n",
    "def get_variable_types(df):\n",
    "    variable_types: dict = {\n",
    "        'Numeric': [],\n",
    "        'Binary': [],\n",
    "        'Categorical': []\n",
    "    }\n",
    "\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == 'boolean':\n",
    "            variable_types['Binary'].append(c)\n",
    "        elif df[c].dtype == 'int8' or df[c].dtype == 'int16' or df[c].dtype == 'int32' or df[c].dtype == 'int64' or \\\n",
    "            df[c].dtype == 'float16' or df[c].dtype == 'float32' or df[c].dtype == 'float64':\n",
    "            variable_types['Numeric'].append(c)\n",
    "        elif df[c].dtype == 'category' or df[c].dtype == 'object':\n",
    "            variable_types['Categorical'].append(c)\n",
    "        else:\n",
    "            print(f'Unknown variable type for {c}')\n",
    "\n",
    "    return variable_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "Rating : 63087 (45.0%)\n",
      "Like : 42814 (30.54%)\n",
      "TestSetId : 97381 (69.46%)\n",
      "RecipeServings : 50021 (35.68%)\n"
     ]
    }
   ],
   "source": [
    "file_source_path = 'dataset_encoded.csv' # source file\n",
    "file_dir = '.' # destination directory\n",
    "file_tag = 'dataset'\n",
    "\n",
    "# Import data\n",
    "df = pd.read_csv(file_source_path, low_memory=False)\n",
    "index_column = df.columns[0]\n",
    "df.drop([index_column], axis=1, inplace=True)\n",
    "\n",
    "# --------------- #\n",
    "# Missing Values  #\n",
    "# --------------- #\n",
    "\n",
    "print(\"Missing values:\")\n",
    "mv = {}\n",
    "for var in df:\n",
    "    nr = df[var].isna().sum()\n",
    "    if nr > 0:\n",
    "        mv[var] = nr\n",
    "        print(f\"{var} : {nr} ({round(nr/df[var].shape[0]*100, 2)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines the number of records to discard entire COLUMNS\n",
    "threshold = df.shape[0] * 0.90\n",
    "\n",
    "# drop columns with more missing values than the defined threshold\n",
    "missings = [c for c in mv.keys() if mv[c]>threshold]\n",
    "df = df.drop(columns=missings, inplace=False)\n",
    "\n",
    "\n",
    "# remove meaningless columns (after manual inspection) \n",
    "df = df.drop(columns=['Rating'], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------- #\n",
    "# APPROACH 2: Fill with MOST FREQ Value after DROP Missing Values #\n",
    "# --------------------------------------------------------------- #\n",
    "\n",
    "# AUX: Fill with MOST FREQUENT value\n",
    "def fill_with_most_frequent(data):\n",
    "    tmp_nr, tmp_cat, tmp_bool = None, None, None\n",
    "    variables = get_variable_types(data.drop(['TestSetId','Like'], axis=1))\n",
    "    numeric_vars = variables['Numeric']\n",
    "    categorical_vars = variables['Categorical']\n",
    "    binary_vars = variables['Binary']\n",
    "\n",
    "    tmp_nr, tmp_cat, tmp_bool = None, None, None\n",
    "    if len(numeric_vars) > 0:\n",
    "        imp = SimpleImputer(strategy='mean', missing_values=nan, copy=True)\n",
    "        tmp_nr = DataFrame(imp.fit_transform(data[numeric_vars]), columns=numeric_vars)\n",
    "    if len(categorical_vars) > 0:\n",
    "        imp = SimpleImputer(strategy='most_frequent', missing_values=nan, copy=True)\n",
    "        tmp_cat = DataFrame(imp.fit_transform(data[categorical_vars]), columns=categorical_vars)\n",
    "    if len(binary_vars) > 0:\n",
    "        imp = SimpleImputer(strategy='most_frequent', missing_values=nan, copy=True)\n",
    "        tmp_bool = DataFrame(imp.fit_transform(data[binary_vars].astype(int)), columns=binary_vars).astype(bool)\n",
    "\n",
    "    df = concat([tmp_nr, tmp_cat, tmp_bool], axis=1)\n",
    "    df['TestSetId'] = data['TestSetId'] ; df['Like'] = data['Like']\n",
    "    df.index = data.index\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------------------------------------------- #\n",
    "\n",
    "# Fill the rest with most frequent value\n",
    "df_most_freq = fill_with_most_frequent(df)\n",
    "df_most_freq.to_csv(f'{file_dir}/{file_tag}_drop_columns_then_most_frequent.csv', index=True)\n",
    "# df_most_freq.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling\n",
    "\n",
    "In contrast to Data Cleaning, Data Wrangling _transforms_ the dataset, in order\n",
    "to prepare it for the training of the models. This includes scaling, dimensionality\n",
    "reduction, data augmentation, outlier removal, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best option: dataset_drop_columns_then_most_frequent\n",
    "file_source_path = 'dataset_drop_columns_then_most_frequent.csv' # source file\n",
    "file_dir = '.' # destination directory\n",
    "file_tag = 'dataset'\n",
    "\n",
    "# read the data\n",
    "df = pd.read_csv(file_source_path, low_memory=False)\n",
    "index_column = df.columns[0]\n",
    "df.drop([index_column], axis=1, inplace=True)\n",
    "\n",
    "# print(df.info())\n",
    "\n",
    "non_numeric_vars =  ['TestSetId', 'AuthorId', 'Diet', 'Name', 'RecipeCategory', 'RecipeIngredientQuantities', \\\n",
    "                     'RecipeIngredientParts', 'RecipeYield', 'HighCalories', 'HighProtein', 'LowFat', \\\n",
    "                     'LowSugar', 'HighFiber', 'Age_30', 'Age_30_60', 'Age_60', 'RespectsRequestedTime', \\\n",
    "                     'Diet_Omnivore', 'Diet_Vegan', 'Diet_Vegetarian', 'MeatMeal', 'VegetarianMeal', 'VeganMeal', \\\n",
    "                     'RecipeCategory_Beverages', 'RecipeCategory_Bread', 'RecipeCategory_Breakfast', \\\n",
    "                     'RecipeCategory_Lunch', 'RecipeCategory_One dish meal', 'RecipeCategory_Soup', 'RecipeCategory_Other', \\\n",
    "                     'MatchNumberOfIngredients', 'RespectsRequestedTime']\n",
    "\n",
    "numeric_vars = get_variable_types(df)['Numeric']\n",
    "# remove original non-numeric variables \n",
    "for var in numeric_vars.copy():\n",
    "    if var in non_numeric_vars:\n",
    "        numeric_vars.remove(var)\n",
    "\n",
    "# Remove class (Like) from numerical variables\n",
    "numeric_vars.remove('Like')\n",
    "\n",
    "summary5 = df.describe(include='number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_outlier_thresholds(summary5, var, OPTION, OUTLIER_PARAM):\n",
    "    # default parameter\n",
    "    if OPTION == 'iqr':\n",
    "        iqr = OUTLIER_PARAM * (summary5[var]['75%'] - summary5[var]['25%'])\n",
    "        top_threshold = summary5[var]['75%'] + iqr\n",
    "        bottom_threshold = summary5[var]['25%'] - iqr\n",
    "    # for normal distribution\n",
    "    elif OPTION == 'stdev':\n",
    "        std = OUTLIER_PARAM * summary5[var]['std']\n",
    "        top_threshold = summary5[var]['mean'] + std\n",
    "        bottom_threshold = summary5[var]['mean'] - std\n",
    "    else:\n",
    "        raise ValueError('Unknown outlier parameter!')\n",
    "    return top_threshold, bottom_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after dropping outliers: (108856, 42)\n"
     ]
    }
   ],
   "source": [
    "# # ------------------------- #\n",
    "# # APPROACH 1: Drop outliers #\n",
    "# # ------------------------- #\n",
    "\n",
    "# # Tuned parameter to get the better results\n",
    "# IQR_PARAM = 2.5\n",
    "\n",
    "# data = df.copy(deep=True)\n",
    "\n",
    "# for var in numeric_vars:\n",
    "#     top_threshold, bottom_threshold = determine_outlier_thresholds(summary5, var, 'iqr', IQR_PARAM)\n",
    "#     outliers = data[(data[var] > top_threshold) | (data[var] < bottom_threshold)]\n",
    "#     # keep all the outliers that have a TestSetId\n",
    "#     outliers = outliers[outliers['TestSetId'].isna()]\n",
    "#     # print(f'{var} outliers: {outliers.shape[0]}/{data[var].shape[0]}')\n",
    "#     data.drop(outliers.index, axis=0, inplace=True)\n",
    "# data.to_csv(f'{file_tag}_drop_outliers_{IQR_PARAM}.csv', index=True)\n",
    "# print('Dataset after dropping outliers:', data.shape)\n",
    "\n",
    "# # Best results: Training score = 0.875\n",
    "\n",
    "# # IQR_PARAM results:\n",
    "# #    IQR    |    1    |   1.5   |    2    |   2.5   |    3    |   3.5   |    4    |\n",
    "# # --------- |---------|---------|---------|---------|---------|---------|---------|\n",
    "# #   Score   |  0.838  |  0.838  |  0.842  |  0.851  |  0.841  |  0.839  |  0.840  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- #\n",
    "# APPROACH 2: Truncate outliers #\n",
    "# ----------------------------- #\n",
    "\n",
    "# Tuned parameter to get the better results\n",
    "IQR_PARAM = 1.5\n",
    "\n",
    "data = df.copy(deep=True)\n",
    "\n",
    "for var in numeric_vars:\n",
    "    top_threshold, bottom_threshold = determine_outlier_thresholds(summary5, var, 'iqr', IQR_PARAM)\n",
    "    # original_column = data[var].copy()\n",
    "    data[var] = data[var].apply(lambda x: top_threshold if x > top_threshold else bottom_threshold if x < bottom_threshold else x)\n",
    "    # print(f'{var} outliers: {(data[var] != original_column).sum()}/{data[var].shape[0]}')\n",
    "data.to_csv(f'{file_tag}_truncate_outliers_{IQR_PARAM}.csv', index=True)\n",
    "print('Dataset after truncating outliers:', data.shape)\n",
    "    \n",
    "# Best results: Score = 0.842 -> IQR_PARAM = 1.5\n",
    "\n",
    "# IQR_PARAM results:\n",
    "#    IQR    |    1    |   1.5   |    2    |   2.5   |    3    |\n",
    "# --------- |---------|---------|---------|---------|---------|\n",
    "#   Score   |  0.842  |  0.842  |  0.834  |  0.834  |  0.831  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "# Best option: dataset_truncate_outliers_1.5\n",
    "file_source_path = 'dataset_truncate_outliers_1.5.csv' # source file\n",
    "file_dir = '.' # destination directory\n",
    "file_tag = 'dataset'\n",
    "\n",
    "# read the data\n",
    "df = pd.read_csv(file_source_path, low_memory=False)\n",
    "index_column = df.columns[0]\n",
    "df = df.drop([index_column], axis=1)\n",
    "\n",
    "variable_types = get_variable_types(df)\n",
    "numeric_vars = variable_types['Numeric']\n",
    "categorical_vars = variable_types['Categorical']\n",
    "boolean_vars = variable_types['Binary']\n",
    "rest_vars = []\n",
    "\n",
    "# print('Numeric variables:', numeric_vars)\n",
    "# print('Categorical variables:', categorical_vars)\n",
    "# print('Boolean variables:', boolean_vars)\n",
    "\n",
    "non_numeric_vars =  ['TestSetId', 'AuthorId', 'Diet', 'Name', 'RecipeCategory', 'RecipeIngredientQuantities', \\\n",
    "                     'RecipeIngredientParts', 'RecipeYield', 'HighCalories', 'HighProtein', 'LowFat', \\\n",
    "                     'LowSugar', 'HighFiber', 'Age_30', 'Age_30_60', 'Age_60', 'RespectsRequestedTime', \\\n",
    "                     'Diet_Omnivore', 'Diet_Vegan', 'Diet_Vegetarian', 'MeatMeal', 'VegetarianMeal', 'VeganMeal', \\\n",
    "                     'RecipeCategory_Beverages', 'RecipeCategory_Bread', 'RecipeCategory_Breakfast', \\\n",
    "                     'RecipeCategory_Lunch', 'RecipeCategory_One dish meal', 'RecipeCategory_Soup', 'RecipeCategory_Other', \\\n",
    "                     'MatchNumberOfIngredients', 'RespectsRequestedTime']\n",
    "\n",
    "# remove original non-numeric variables \n",
    "for var in numeric_vars.copy():\n",
    "    if var in non_numeric_vars:\n",
    "        numeric_vars.remove(var)\n",
    "        rest_vars.append(var)\n",
    "\n",
    "# Remove class (Like) from numerical variables\n",
    "numeric_vars.remove('Like')\n",
    "\n",
    "df_num = df[numeric_vars]\n",
    "df_symb = df[categorical_vars]\n",
    "df_bool = df[boolean_vars]\n",
    "df_rest = df[rest_vars]\n",
    "df_target = df['Like']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MinMax**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- #\n",
    "# MinMax normalization  #\n",
    "# --------------------- #\n",
    "\n",
    "transf = MinMaxScaler(feature_range=(0, 1), copy=True).fit(df_num)\n",
    "tmp = DataFrame(transf.transform(df_num), index=df.index, columns= numeric_vars)\n",
    "temp_norm_data_minmax = concat([tmp, df_symb, df_bool, df_rest], axis=1)\n",
    "norm_data_minmax = concat([temp_norm_data_minmax, df_target], axis=1)\n",
    "norm_data_minmax.to_csv(f'{file_tag}_scaled_minmax.csv', index=True)\n",
    "# print(norm_data_minmax.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra:** Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Best option: dataset_rating_drop_recipe_mean\n",
    "# file_source_path = 'datasets/scaling/dataset_dataset_drop_outliers_1.5_scaled_minmax.csv' # source file\n",
    "\n",
    "# # read the data\n",
    "# df = pd.read_csv(file_source_path, low_memory=False)\n",
    "# convert_variable_types(df)\n",
    "# index_column = df.columns[0]\n",
    "# df.drop([index_column], axis=1, inplace=True)\n",
    "\n",
    "# # We can also leverage the dataprep package to get a nice summary report\n",
    "# report = sv.analyze(df)\n",
    "# report.show_notebook()\n",
    "\n",
    "# # We can also leverage the yadata_profiling package to get a nice summary report\n",
    "# profile = ProfileReport(df, title=\"LLMeals - Summary Report\")\n",
    "# profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best option: dataset_scaled_minmax\n",
    "file_source_path = 'dataset_scaled_minmax.csv' # source file\n",
    "file_tag = 'dataset'\n",
    "\n",
    "# read the data\n",
    "df = pd.read_csv(file_source_path, low_memory=False)\n",
    "index_column = df.columns[0]\n",
    "df.drop([index_column], axis=1, inplace=True)\n",
    "\n",
    "variable_types = get_variable_types(df)\n",
    "numeric_vars = variable_types['Numeric']\n",
    "\n",
    "# print('Numeric variables:', numeric_vars)\n",
    "\n",
    "non_numeric_vars =  ['TestSetId', 'AuthorId', 'Diet', 'Name', 'RecipeCategory', 'RecipeIngredientQuantities', \\\n",
    "                     'RecipeIngredientParts', 'RecipeYield', 'HighCalories', 'HighProtein', 'LowFat', \\\n",
    "                     'LowSugar', 'HighFiber', 'Age_30', 'Age_30_60', 'Age_60', 'RespectsRequestedTime', \\\n",
    "                     'Diet_Omnivore', 'Diet_Vegan', 'Diet_Vegetarian', 'MeatMeal', 'VegetarianMeal', 'VeganMeal', \\\n",
    "                     'RecipeCategory_Beverages', 'RecipeCategory_Bread', 'RecipeCategory_Breakfast', \\\n",
    "                     'RecipeCategory_Lunch', 'RecipeCategory_One dish meal', 'RecipeCategory_Soup', 'RecipeCategory_Other', \\\n",
    "                     'MatchNumberOfIngredients', 'RespectsRequestedTime']\n",
    "\n",
    "# remove original non-numeric variables \n",
    "for var in numeric_vars.copy():\n",
    "    if var in non_numeric_vars:\n",
    "        numeric_vars.remove(var)\n",
    "\n",
    "# Remove class (Like) from categorical variables\n",
    "numeric_vars.remove('Like')\n",
    "\n",
    "df_num = df[numeric_vars]\n",
    "\n",
    "# ---------------------------- #\n",
    "# Dropping Redundant Variables #\n",
    "# ---------------------------- #\n",
    "\n",
    "THRESHOLD = 0.9\n",
    "\n",
    "def select_redundant(corr_mtx, threshold):\n",
    "    if corr_mtx.empty:\n",
    "        return {}\n",
    "\n",
    "    corr_mtx = abs(corr_mtx)\n",
    "    vars_2drop = {}\n",
    "    for el in corr_mtx.columns:\n",
    "        el_corr = (corr_mtx[el]).loc[corr_mtx[el] >= threshold]\n",
    "        if len(el_corr) == 1:\n",
    "            corr_mtx.drop(labels=el, axis=1, inplace=True)\n",
    "            corr_mtx.drop(labels=el, axis=0, inplace=True)\n",
    "        else:\n",
    "            vars_2drop[el] = el_corr.index\n",
    "    return vars_2drop, corr_mtx\n",
    "\n",
    "drop, corr_mtx = select_redundant(df_num.corr(), THRESHOLD)\n",
    "\n",
    "if corr_mtx.empty:\n",
    "    print('Matrix is empty. No redundant variables to drop.')\n",
    "\n",
    "# figure(figsize=[12, 12])\n",
    "# heatmap(corr_mtx, xticklabels=corr_mtx.columns, yticklabels=corr_mtx.columns, annot=False, cmap='Blues')\n",
    "# title('Filtered Correlation Analysis')\n",
    "# tight_layout()\n",
    "# show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_redundant(data, vars_2drop):\n",
    "    sel_2drop = []\n",
    "    for key in vars_2drop.keys():\n",
    "        if key not in sel_2drop:\n",
    "            for r in vars_2drop[key]:\n",
    "                if r != key and r not in sel_2drop:\n",
    "                    sel_2drop.append(r)\n",
    "    # print('Variables to drop: ', sel_2drop)\n",
    "    df = data.copy()\n",
    "    for var in sel_2drop:\n",
    "        df.drop(labels=var, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# print(\"Variables before drop: \", drop.keys())\n",
    "# df = drop_redundant(df, drop)\n",
    "\n",
    "df.to_csv(f'{file_tag}_selected.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "We experimented with applying PCA to the dataset to reduce its dimensionality while retaining as much of the original information as possible. However, instead of yielding better results, the outcomes worsened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a cleaned data set, and before we start the *Modelling* phase, we are going to split our data set into multiple sub-datasets. \n",
    "Here, we are going to balance the data to ensure that both classes are equally represented, and then split it into an *train* and *test* data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_source_path = 'dataset_selected.csv' # source file\n",
    "target = 'Like'\n",
    "\n",
    "# read data\n",
    "df = pd.read_csv(f'{file_source_path}', low_memory=False)\n",
    "# remove index column\n",
    "index_column = df.columns[0]\n",
    "df = df.drop([index_column], axis=1)\n",
    "# Drop TestSetId column and Like NaN rows (no way to know if they liked or not)\n",
    "df.drop('TestSetId', axis=1, inplace=True)\n",
    "df.dropna(subset=[target], inplace=True)\n",
    "\n",
    "# print(df.shape)\n",
    "# Take a random sample of 10k rows\n",
    "# df_test = df.sample(n=int(df.shape[0]/10)).copy(deep=True)\n",
    "# df_test.to_csv(f'df_test.csv', index=False)\n",
    "# df.drop(df_test.index, axis=0, inplace=True)\n",
    "# print(df.shape)\n",
    "\n",
    "# ----------------------------- #\n",
    "#           BALANCING           #\n",
    "# ----------------------------- #\n",
    "like_zero = df[df[target] == 0.0]\n",
    "like_one = df[df[target] == 1.0]\n",
    "\n",
    "df_one_sample = like_one.sample(len(like_zero), replace=True)\n",
    "df_zero_sample = like_zero.sample(len(like_zero))\n",
    "\n",
    "df = pd.concat([df_zero_sample, df_one_sample], axis=0)\n",
    "# ----------------------------- #\n",
    "\n",
    "y = df.pop(target).values\n",
    "X = df.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y,\n",
    "                test_size=0.3, \n",
    "                shuffle=True,\n",
    "                random_state=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'> Phase 4: Modeling </font>\n",
    "\n",
    "In this phase, the model is trained and tuned. In general, data transformations\n",
    "from data wrangling can be part of a machine learning pipeline, and can therefore\n",
    "be tuned as well. (See CRISP-DM: DataPrep <--> Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter: {'model': HistGradientBoostingClassifier(max_depth=16), 'model__max_depth': 16} (CV score=0.870)\n"
     ]
    }
   ],
   "source": [
    "# Here, we want to find the best classifier. As candidates, we consider\n",
    "#   1. LogisticRegression\n",
    "#   2. RandomForestClassifier\n",
    "#   3. GradientBoostingClassifier\n",
    "#   4. HistGradientBoostingClassifier\n",
    "#   5. AdaBoostClassifier\n",
    "#   6. MLPClassifier\n",
    "    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, HistGradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "model_logistic_regression = LogisticRegression(max_iter=300)\n",
    "model_random_forest = RandomForestClassifier()\n",
    "model_gradient_boosting = GradientBoostingClassifier()\n",
    "model_adaboost = AdaBoostClassifier()\n",
    "model_hist_gradient_boosting = HistGradientBoostingClassifier()\n",
    "model_neural_network = MLPClassifier(max_iter=200)\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=[(\"model\", None)])\n",
    "\n",
    "parameter_grid_preprocessing = {\n",
    "  # Empty on purpose (no preprocessing)\n",
    "}\n",
    "\n",
    "# NOTE: Logistic Regression does not perform as well as the other models\n",
    "parameter_grid_logistic_regression = {\n",
    "  \"model\" : [model_logistic_regression],\n",
    "  \"model__C\" : [0.1, 1, 10],  # inverse regularization strength\n",
    "}\n",
    "\n",
    "parameter_grid_gradient_boosting = {\n",
    "  \"model\" : [model_gradient_boosting],\n",
    "  \"model__n_estimators\" : [150],\n",
    "  \"model__max_depth\" : list(range(df.shape[1]-12, df.shape[1]+1)), # 4\n",
    "  # \"model__learning_rate\" : [0.2],\n",
    "}\n",
    "\n",
    "# This estimator is much faster than GradientBoostingClassifier for big datasets (n_samples >= 10 000).\n",
    "parameter_grid_hist_gradient_boosting = {\n",
    "  \"model\" : [model_hist_gradient_boosting],\n",
    "  # \"model__learning_rate\" : [0.2], \n",
    "  \"model__max_depth\" : list(range(15, df.shape[1]+1)), # 4\n",
    "}\n",
    "\n",
    "parameter_grid_adaboost = {\n",
    "  \"model\" : [model_adaboost],\n",
    "  \"model__n_estimators\" : [150]\n",
    "}\n",
    "\n",
    "parameter_grid_random_forest = {\n",
    "  \"model\" : [model_random_forest],\n",
    "  \"model__n_estimators\" : [10, 20, 50],  # number of max trees in the forest\n",
    "  \"model__max_depth\" : [5, 10, 15],\n",
    "}\n",
    "\n",
    "# NOTE: NN does not perform well on this dataset + takes a long time to train\n",
    "parameter_grid_neural_network = {\n",
    "  \"model\": [model_neural_network],\n",
    "  \"model__hidden_layer_sizes\": [(30, 30), (40, 30)],  # Example hidden layer configurations\n",
    "  \"model__alpha\": [0.0001],  # Regularization parameter\n",
    "}\n",
    "\n",
    "\n",
    "meta_parameter_grid = [\n",
    "                      # parameter_grid_logistic_regression,\n",
    "                      #  parameter_grid_random_forest] #,\n",
    "                      #  parameter_grid_gradient_boosting] #,\n",
    "                       parameter_grid_hist_gradient_boosting] #,\n",
    "                      #  parameter_grid_adaboost ] #,\n",
    "                      #  parameter_grid_neural_network]\n",
    "\n",
    "meta_parameter_grid = [{**parameter_grid_preprocessing, **model_grid}\n",
    "                       for model_grid in meta_parameter_grid]\n",
    "\n",
    "search = GridSearchCV(pipeline,\n",
    "                      meta_parameter_grid, \n",
    "                      scoring=\"balanced_accuracy\",\n",
    "                      n_jobs=-1, \n",
    "                      cv=5,  # number of folds for cross-validation \n",
    "                      error_score=\"raise\"\n",
    ")\n",
    "\n",
    "# here, the actual training and grid search happens\n",
    "search.fit(X_train, y_train.ravel())\n",
    "\n",
    "print(\"best parameter:\", search.best_params_ ,\"(CV score=%0.3f)\" % search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'> Step 5: Evaluation </font>\n",
    "\n",
    "Once the appropriate models are chosen, they are evaluated on the test set. For\n",
    "this, different evaluation metrics can be used. Furthermore, this step is where\n",
    "the models and their predictions are analyzed resp. different properties, including\n",
    "feature importance, robustness to outliers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on test set: 0.8737101546405752\n",
      "true    0.0    1.0\n",
      "pred              \n",
      "0.0   14548   1378\n",
      "1.0    3059  16100\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance of model on test set\n",
    "print(\"Score on test set:\", search.score(X_test, y_test.ravel()))\n",
    "\n",
    "# contingency table\n",
    "ct = pd.crosstab(search.best_estimator_.predict(X_test), y_test.ravel(),\n",
    "                 rownames=[\"pred\"], colnames=[\"true\"])\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for a detailed look on the performance of the different models (if different models are used)\n",
    "# def get_search_score_overview():\n",
    "#   for c,s in zip(search.cv_results_[\"params\"],search.cv_results_[\"mean_test_score\"]):\n",
    "#       print(c, s)\n",
    "\n",
    "# print(get_search_score_overview())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'> Step 6: Deployment </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------------------------------------------- #\n",
    "# # TEST with a random sample of 10k rows from the original dataset   #\n",
    "# # ----------------------------------------------------------------- #\n",
    "\n",
    "# # read data\n",
    "# df_test = pd.read_csv('df_test.csv', low_memory=False)\n",
    "\n",
    "# def micro_service_classify_review_test(datapoint):\n",
    "#   # make sure the provided datapoints adhere to the correct format for model input\n",
    "  \n",
    "#   # fetch your trained model\n",
    "#   model = search.best_estimator_\n",
    "\n",
    "#   # make prediction with the model\n",
    "#   prediction = model.predict(datapoint)\n",
    "\n",
    "#   return prediction\n",
    "\n",
    "# # Save the Like values in a vector\n",
    "# like_values = df_test['Like'].values\n",
    "\n",
    "# # Optionally, you can drop the 'Like' column from the sampled_df if you don't need it\n",
    "# df_test.drop('Like', axis=1, inplace=True)\n",
    "\n",
    "# # make the missing predictions for the Like column\n",
    "# df_test['Like'] = micro_service_classify_review_test(df_test.values)\n",
    "\n",
    "# # Calculate balanced accuracy\n",
    "# balanced_acc = balanced_accuracy_score(like_values, df_test['Like'])\n",
    "\n",
    "# print(f\"Balanced Accuracy: {balanced_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_service_classify_review(datapoint):\n",
    "  # 'TestSetId' is not a feature used for prediction\n",
    "  datapoint = datapoint.drop('TestSetId', axis=1)\n",
    "\n",
    "  # fetch your trained model\n",
    "  model = search.best_estimator_\n",
    "\n",
    "  # make prediction with the model\n",
    "  prediction = model.predict(datapoint)\n",
    "\n",
    "  return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Analytics Cup, we need to export your prediction in a very specific output format. This is a csv file without an index and two columns, *id* and *prediction*. Note that the values in both columns need to be integer values, and especially in the *prediction* column either 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Predictions saved to predictions_tugas.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X has feature names, but HistGradientBoostingClassifier was fitted without feature names\n"
     ]
    }
   ],
   "source": [
    "file_source_path = 'dataset_selected.csv' # source file\n",
    "\n",
    "# read data\n",
    "df = pd.read_csv(f'{file_source_path}', low_memory=False)\n",
    "# remove index column\n",
    "index_column = df.columns[0]\n",
    "df = df.drop([index_column], axis=1)\n",
    "\n",
    "# keep only the rows without a Like value\n",
    "df = df[df['Like'].isna()]\n",
    "# remove the Like column\n",
    "df.drop('Like', axis=1, inplace=True)\n",
    "\n",
    "# make the missing predictions for the Like column\n",
    "df['Like'] = micro_service_classify_review(df)\n",
    "\n",
    "# create a dataset that contains only the column \n",
    "# with the TestSetId and the model prediction for Like\n",
    "output = df[['TestSetId', 'Like']]\n",
    "\n",
    "# rename the columns to match the required format\n",
    "output = output.rename(columns={'TestSetId': 'id'})\n",
    "output = output.rename(columns={'Like': 'prediction'})\n",
    "submission = output.reindex(columns=[\"id\", \"prediction\"])\n",
    "# convert id and prediction to integer\n",
    "submission['id'] = submission['id'].astype(int)\n",
    "submission['prediction'] = submission['prediction'].astype(int)\n",
    "\n",
    "# print(submission.head())\n",
    "\n",
    "# save the submission to a CSV file\n",
    "submission.to_csv('predictions_tugas.csv', index = False)\n",
    "print(\"- Predictions saved to predictions_tugas.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
